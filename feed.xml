<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iampukar.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iampukar.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-10T03:57:14+00:00</updated><id>https://iampukar.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Unlocking the Power of Uniswap V4 Hooks</title><link href="https://iampukar.github.io/blog/2024/uniswap-v4-hooks/" rel="alternate" type="text/html" title="Unlocking the Power of Uniswap V4 Hooks"/><published>2024-10-10T11:46:00+00:00</published><updated>2024-10-10T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/uniswap-v4-hooks</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/uniswap-v4-hooks/"><![CDATA[<h2 id="introduction-uniswap-hooks-and-beyond">Introduction: Uniswap Hooks and Beyond</h2> <p>Uniswap has continuously evolved, driving the decentralized finance (DeFi) revolution through its different versions. Each upgrade brought groundbreaking features: Uniswap V2 laid the foundation of automated market makers, V3 introduced innovations like concentrated liquidity, and now V4 redefines customization with hooks. Hooks provide an unprecedented level of control and personalization in liquidity pools, allowing developers to extend the functionality of Uniswap beyond its core features.</p> <p>In this blog, we‚Äôll explore how Uniswap has evolved to meet the needs of liquidity providers (LPs) and developers alike. We‚Äôll delve into Hooks as the central focus, with detailed explanations of their implementation and customization potential. Alongside Hooks, we‚Äôll cover related features like Concentrated Liquidity, Ticks, and sqrtPriceLimitX96‚Äîall of which play crucial roles in optimizing capital efficiency.</p> <h2 id="concentrated-liquidity-in-uniswap">Concentrated Liquidity in Uniswap</h2> <p>Concentrated liquidity is a revolutionary feature introduced in Uniswap V3, allowing liquidity providers (LPs) to designate a specific price range within which their liquidity is active. This change significantly improves capital efficiency compared to Uniswap V2, where liquidity was spread uniformly across the entire price curve from zero to infinity (0, ‚àû).</p> <p>In simple terms, LPs can now focus their liquidity on a specific price range, like setting up a vending stall only in areas with the highest demand. For example, an LP providing liquidity for the ETH/USDT pair might concentrate their assets within the $1,500 to $2,000 price range. This ensures that liquidity is deployed where it is most likely to be used, increasing the potential for earning fees.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/other_support_images/uniswap-liquidity-distribution-480.webp 480w,/assets/img/other_support_images/uniswap-liquidity-distribution-800.webp 800w,/assets/img/other_support_images/uniswap-liquidity-distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/other_support_images/uniswap-liquidity-distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Liquidity Distribution as Proposed by Uniswap V3. Courtesy of <a href="https://uniswap.org/whitepaper-v3.pdf" target="_blank">Uniswap V3 Whitepaper</a>. </div> <h3 id="capital-efficiency-and-yield-considerations">Capital Efficiency and Yield Considerations</h3> <p>The main advantage of concentrated liquidity is improved capital efficiency. LPs can provide the same depth of liquidity as they would in Uniswap V2, but with significantly less capital. This also makes it easier for smaller LPs to compete with larger players by strategically positioning their liquidity.</p> <p>However, the trade-off is the need for active management. If the price moves outside the specified range, the liquidity becomes inactive, and LPs earn no fees until the price returns. This makes liquidity provision in Uniswap V3 more like active portfolio management rather than a passive approach.</p> <p>To bridge the technical depth, let‚Äôs simplify: Concentrated liquidity allows LPs to make their capital work harder for them by narrowing down their focus. Think of it as betting on a horse race‚Äîwe‚Äôre placing our bets on the most likely winning stretch of prices rather than betting on every possible outcome.</p> <h3 id="gas-costs">Gas Costs</h3> <p>Concentrated liquidity does come with increased gas costs due to the complexity of managing specific price ranges. LPs need to consider whether the potential earnings from concentrated liquidity justify the higher gas fees. For smaller-scale liquidity provision, the benefits of capital efficiency may be offset by these additional costs.</p> <p>To put it in simpler terms: The efficiency gains come at a price‚Äîhigher fees when adding or removing liquidity. This is something every LP needs to balance based on their investment size.</p> <h2 id="ticks-discretizing-the-price-range-in-uniswap-v3">Ticks: Discretizing the Price Range in Uniswap V3</h2> <p>Ticks are discrete price points that define the boundaries of liquidity ranges within a Uniswap V3 pool. They enable LPs to concentrate liquidity within specific price intervals, enhancing precision and capital efficiency.</p> <h3 id="what-are-ticks">What Are Ticks?</h3> <p>Ticks represent specific price levels, calculated as an integer power of 1.0001. This allows LPs to define a price range for their liquidity. For technical efficiency, Uniswap uses the square root of these prices in its internal calculations.</p> <p>To make it more relatable: Imagine ticks as units on a ruler that divides a price range into manageable parts, allowing LPs to precisely place their bets on which segment of the range they think is most profitable.</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">üìä Ticks: Uniswap v3 divides the price range into discrete intervals called ticks. Each tick represents a specific price point. <a href="https://t.co/Cd9tEJSrBB">pic.twitter.com/Cd9tEJSrBB</a></p>&mdash; Panoptic (@Panoptic_xyz) <a href="https://twitter.com/Panoptic_xyz/status/1813289804295209238?ref_src=twsrc%5Etfw">July 16, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Imagine a vendor setting up goods in specific sections of a street market. Ticks help LPs do the same with their liquidity‚Äîfocusing on price ranges where they expect the most demand. For instance, an LP providing liquidity for the ETH/USDC pair might focus on the $1,500 to $1,600 range, ensuring efficient liquidity deployment when the price stays within this range.</p> <h3 id="advantages-of-ticks">Advantages of Ticks</h3> <ul> <li><strong>Capital Efficiency</strong>: LPs can achieve higher returns by focusing liquidity on price ranges with high trading activity.</li> <li><strong>Precision in Strategies</strong>: Ticks allow LPs to fine-tune their positions, adopting strategies tailored to their risk tolerance and market outlook.</li> <li><strong>Enhanced Price Oracle</strong>: Ticks improve the accuracy of price oracles, which is beneficial for other DeFi protocols relying on precise price feeds.</li> </ul> <h2 id="understanding-sqrtpricelimitx96-in-uniswap-v3">Understanding sqrtPriceLimitX96 in Uniswap V3</h2> <p><code class="language-plaintext highlighter-rouge">sqrtPriceLimitX96</code> is a fixed-point representation of the square root of a price, used as a price limit for certain operations. This parameter is crucial for managing liquidity ranges and preventing excessive slippage during trades.</p> <p>For example, an LP providing liquidity for the ETH/USDC pool might only want their liquidity to be active when the price is between $1,500 and $1,600. They use <code class="language-plaintext highlighter-rouge">sqrtPriceLimitX96</code> values to set these boundaries, ensuring their liquidity is deployed efficiently within this specific range.</p> <p>In simpler terms: <code class="language-plaintext highlighter-rouge">sqrtPriceLimitX96</code> is like setting guardrails on our liquidity position. It ensures that we‚Äôre <a href="https://docs.uniswap.org/contracts/v3/reference/core/UniswapV3Pool#parameters-7">only active in a predetermined safe zone</a>, which helps avoid risks from unexpected price moves.</p> <h3 id="technical-insights">Technical Insights</h3> <p>Uniswap uses Q notation to represent fractional values that Solidity can handle. Specifically, <code class="language-plaintext highlighter-rouge">sqrtPriceLimitX96</code> uses Q96 notation, which involves multiplying the value by <code class="language-plaintext highlighter-rouge">2^96</code> to maintain precision. This approach allows Uniswap to manage price increments accurately, ensuring efficient liquidity management.</p> <h2 id="understanding-hooks-in-uniswap-v4">Understanding Hooks in Uniswap V4</h2> <p>Hooks are customizable plugins within the <a href="https://raw.githubusercontent.com/Uniswap/v4-core/main/docs/whitepaper/whitepaper-v4.pdf">Uniswap V4</a> protocol. They allow developers to inject custom logic into the execution flow of a pool, providing flexibility beyond the core functionality of Uniswap.</p> <h3 id="how-hooks-work">How Hooks Work</h3> <p>Hooks are externally deployed contracts that interact with the core logic of a Uniswap pool. When a pool is created, developers can specify a hook contract, which defines specific functions that are triggered at key events, such as swaps or liquidity changes.</p> <h3 id="practical-applications-of-hooks">Practical Applications of Hooks</h3> <ul> <li><strong>Dynamic Fee Management</strong>: Hooks can adjust swap fees based on market conditions. For example, during high volatility, a hook could increase fees to mitigate risk for LPs.</li> <li><strong>Custom Trading Curves</strong>: Developers can create custom Automated Market Maker (AMM) curves, such as constant sum curves for stablecoin pairs, using hooks.</li> <li><strong>Fee Redirection for Social Impact</strong>: Hooks can be used to redirect a portion of trading fees to charitable causes, adding a social impact component to liquidity provision.</li> </ul> <h3 id="detailed-example-of-hook-implementation-take-profit-orders">Detailed Example of Hook Implementation: Take-Profit Orders</h3> <p>A Take-Profit Order is a type of limit-order that specifies the exact price at which a trader wishes to close their open position for a profit. The order is only filled if the price of the asset reaches the limit price, otherwise it remains open.</p> <p>For example, imagine ETH is currently trading in the ETH/DAI pool at 1 ETH = 1,500 DAI. We could place a take-profit order that essentially says, ‚ÄúSell all my ETH if 1 ETH = 2,000 DAI.‚Äù If and when that price is hit, our ETH will automatically be swapped over to DAI completely on-chain in a decentralized way.</p> <p><strong>Step-by-Step Implementation Overview:</strong></p> <ol> <li><strong>Create Hook Contract</strong>: We need a smart contract that defines the logic for managing take-profit orders. This includes functions for placing, canceling, and redeeming orders.</li> <li><strong>Place Order</strong>: Users place an order by interacting with the hook contract. The contract mints ERC-1155 tokens representing their order, which serve as receipts for the user. These tokens can be traded or transferred while the order is open.</li> <li><strong>Cancel Order:</strong> Users can cancel their order at any time, and the corresponding ERC-1155 tokens are burned. The user‚Äôs assets are returned without the order being filled.</li> <li><strong>afterSwap Hook:</strong> Each time a swap occurs in the pool, the afterSwap hook is triggered. The contract checks the current price against open take-profit orders. If the target price is reached, the order is automatically filled, and the user‚Äôs tokens are swapped.</li> <li><strong>Fill Order:</strong> To fill an order, the hook prepares the swap parameters, such as sqrtPriceLimitX96, and handles the swap. The logic includes tracking order type, maximum slippage, and ensuring that all calculations are accurate.</li> <li><strong>Redeem Tokens:</strong> Once the order is filled, users can redeem their swapped tokens by returning the ERC-1155 receipt tokens to the contract. The contract burns these tokens and transfers the swapped assets to the user.</li> </ol> <p>A detailed example of the take-profit hook can be found <a href="https://learnweb3.io/lessons/uniswap-v4-hooks-create-a-fully-on-chain-take-profit-orders-hook-on-uniswap-v4/">here</a>. Additionally, you can also refer to this <a href="https://github.com/ora-io/awesome-uniswap-hooks">curated lists of awesome hooks</a> resources.</p> <h3 id="technical-insights-into-hooks">Technical Insights into Hooks</h3> <p>Hooks are designed to be modular and easily integrated. This allows developers to iterate on their hook logic separately from the core protocol, providing additional security and flexibility. For example, dynamic fee adjustments based on external Oracle data or changing AMM behavior based on market volatility can all be seamlessly integrated with Hooks.</p> <p>The Take-Profit Hook leverages the afterSwap hook to monitor each swap event and compare the new price against existing orders. This modularity ensures that any price movement in the pool is captured, allowing pending orders to be filled automatically.</p> <h2 id="conclusion">Conclusion</h2> <p>Each version of Uniswap has brought unique innovations that have revolutionized decentralized finance. The concentrated liquidity and ticks features from V3 allow for greater capital efficiency, while hooks from its V4 version advancements provide unprecedented customization for developers and liquidity providers. By understanding these features, LPs can make informed decisions about how to deploy their capital most effectively, whether they prefer passive strategies or more active management.</p> <p>In a nutshell, this evolution showcases the power of customization, efficiency, and flexibility. Hooks, in particular, open up new possibilities for developers to add personalized behavior to liquidity pools, making Uniswap more adaptable and community-oriented than ever.</p>]]></content><author><name></name></author><category term="uniswap"/><category term="defi"/><category term="uniswap"/><category term="uniswap-hooks"/><category term="concentrated-liquidity"/><category term="deFi"/><summary type="html"><![CDATA[Discover how Uniswap V4 Hooks transform DeFi by enabling advanced customization and liquidity management, with insights on concentrated liquidity, ticks, and capital efficiency.]]></summary></entry><entry><title type="html">Preprocessing Unstructured Data for LLM Applications</title><link href="https://iampukar.github.io/blog/2024/preprocessing-unstructured-data-for-llm-applications/" rel="alternate" type="text/html" title="Preprocessing Unstructured Data for LLM Applications"/><published>2024-09-27T11:46:00+00:00</published><updated>2024-09-27T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/preprocessing-unstructured-data-for-llm-applications</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/preprocessing-unstructured-data-for-llm-applications/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In the era of advanced AI applications, <strong>preprocessing unstructured data</strong> is the key to unlocking the full potential of <strong>large language models (LLMs)</strong>. With the increasing demand for applications like <strong>Retrieval Augmented Generation (RAG)</strong> systems, effective handling of data from multiple formats‚Äîsuch as PDFs, PowerPoint presentations, Markdown files, and even images‚Äîhas become critical. Preprocessing not only enables LLMs to interact with external data sources but also ensures that the information is retrieved and presented in a meaningful way, enhancing accuracy and relevance.</p> <p>This blog will take us through the essential techniques and strategies for preprocessing unstructured data, including <strong>metadata extraction</strong>, <strong>chunking</strong>, and the use of <strong>document layout models</strong> and <strong>vision transformers</strong>. Whether we‚Äôre working on business reports, academic papers, or large datasets, mastering these techniques will allow us to build more efficient, scalable, and responsive LLM applications.</p> <h2 id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h2> <h3 id="what-is-retrieval-augmented-generation">What is Retrieval Augmented Generation?</h3> <p><strong>Retrieval Augmented Generation (RAG)</strong> is a powerful approach that enhances <strong>large language models (LLMs)</strong> by grounding their responses on <strong>external, validated information</strong>. Instead of relying solely on pre-trained data, RAG applications retrieve relevant data from external sources such as emails, PDFs, or PowerPoint slides. This information is then integrated into the prompt passed to the LLM, resulting in more <strong>accurate</strong> and <strong>context-rich</strong> responses.</p> <p>In a typical RAG pipeline, external data is stored in a <strong>vector database</strong>. When a query is made, relevant data is retrieved, <strong>chunked</strong>, and <strong>embedded</strong> into the prompt. This ensures that the LLM can draw on <strong>current</strong> and <strong>relevant information</strong> instead of solely relying on pre-existing knowledge.</p> <h3 id="preprocessing-output">Preprocessing Output</h3> <p><strong>Preprocessing unstructured data</strong> is essential for preparing content for RAG pipelines. We break down documents into manageable <strong>document elements</strong> and capture <strong>element data</strong> to better organize and retrieve information.</p> <p><strong>Document Elements</strong></p> <p>Key document elements extracted during preprocessing are crucial to ensure that the right information is used in the LLM‚Äôs responses:</p> <ul> <li><strong>Title:</strong> The main heading or topic of a document section.</li> <li><strong>Narrative Text:</strong> The core body content containing detailed explanations or descriptions.</li> <li><strong>List Item:</strong> Structured content organized as bulleted or numbered lists.</li> <li><strong>Table:</strong> Data arranged in rows and columns, often representing quantitative or categorical information.</li> <li><strong>Image:</strong> Visual elements or graphics embedded within the document.</li> </ul> <p><strong>Element Data</strong></p> <p>For each document element, element data provides additional context, helping with <strong>filtering</strong> and <strong>organizing</strong> content. This data includes:</p> <ul> <li><strong>Filename:</strong> The name of the source document.</li> <li><strong>Filetype:</strong> The type of document (e.g., PDF, PowerPoint, HTML).</li> <li><strong>Page Number:</strong> The page on which the element resides, especially relevant for multi-page documents like PDFs.</li> <li><strong>Section:</strong> The specific section or chapter where the element is located.</li> </ul> <p>This element data plays an important role in ensuring efficient information retrieval in RAG applications.</p> <h3 id="why-is-preprocessing-hard">Why is Preprocessing Hard?</h3> <p>Preprocessing for LLMs in RAG pipelines presents several challenges due to the diversity in file formats, structures, and the nuances of interpreting different types of content. Let‚Äôs explore the key reasons why preprocessing is complex:</p> <p><strong>Content Cues</strong></p> <p>Different file formats provide different content cues that help signal the structure of the document. Interpreting these cues across various formats is crucial for accurate data extraction:</p> <ul> <li><strong>HTML</strong> files use structured tags (e.g., <code class="language-plaintext highlighter-rouge">&lt;h1&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;ul&gt;</code>) to define elements like titles, lists, or tables.</li> <li><strong>PDFs</strong> rely on <strong>visual cues</strong> such as font size, boldness, and spacing to indicate hierarchy, like headings and subheadings.</li> </ul> <p><strong>Standardization Need</strong></p> <p>Each file format‚Äîwhether <strong>PDF</strong>, <strong>Markdown</strong>, or <strong>HTML</strong>‚Äîpresents information differently. To ensure consistent processing, it is vital to <strong>standardize the output</strong> across all formats. Standardization ensures that our application can handle content uniformly, regardless of its source. However, this is a difficult task due to the <strong>variability</strong> in document structures and how they indicate content.</p> <p><strong>Extraction Visibility</strong></p> <p>At times, important information is hidden within the document in ways that aren‚Äôt immediately visible in the raw content. For example, tables in PDFs or lists in PowerPoints might not be accessible as plain text, requiring <strong>specialized extraction techniques</strong>. These hidden elements further complicate preprocessing.</p> <p><strong>Metadata Insights</strong></p> <p>In addition to raw content extraction, gathering <strong>metadata</strong> such as file type, page number, and section is essential for operations like <strong>filtering</strong> and <strong>hybrid search</strong> in RAG applications. However, extracting and understanding metadata is challenging due to the <strong>different internal representations</strong> of various file formats.</p> <h2 id="normalizing-diverse-document-types">Normalizing Diverse Document Types</h2> <p>In developing applications powered by <strong>large language models (LLMs)</strong>, normalizing content from various document types is critical for efficient data processing. Whether we‚Äôre handling <strong>PDFs</strong>, <strong>PowerPoint presentations</strong>, <strong>Word documents</strong>, or <strong>HTML pages</strong>, normalization ensures that all formats are processed uniformly. This simplifies downstream operations, reduces complexity, and makes the entire pipeline more cost-effective.</p> <h3 id="why-normalize-diverse-document-types">Why Normalize Diverse Document Types?</h3> <p>Documents come in a variety of formats, each with its own structure and characteristics. Managing these diverse formats in an LLM application can be challenging. The goal of <strong>normalization</strong> is to standardize this process, ensuring that all document types are broken down into <strong>common elements</strong> like titles, narrative text, or lists. This enables the LLM to process them uniformly, regardless of the source format.</p> <h3 id="benefits-of-normalization">Benefits of Normalization</h3> <ol> <li><strong>Uniform Processing Across Formats:</strong> Once documents are normalized, they can be processed with the same code, whether the source is a PDF, PowerPoint, or HTML file. For example, after normalization, we can filter out headers or footers uniformly, eliminating the need for separate logic for each file type.</li> <li><strong>Consistent Chunking and Cost Reduction:</strong> Normalized documents allow for consistent <strong>chunking</strong> operations across all formats, reducing processing complexity and cost. Content extraction is typically the most resource-intensive part of preprocessing, while chunking is less costly. Standardizing document structure enables more efficient experimentation with different chunking strategies, cutting down on computational overhead.</li> </ol> <h3 id="data-serialization">Data Serialization</h3> <p>Once content is normalized, the next critical step is <strong>data serialization</strong>, which ensures that the preprocessed content can be reused later. <strong>JSON</strong> is a popular and versatile format for serialization due to several advantages:</p> <ol> <li><strong>Common and Well-understood Structure:</strong> JSON is universally recognized and widely supported across platforms and systems.</li> <li><strong>Standard HTTP Response:</strong> JSON is natively supported as a <strong>standard HTTP response format</strong>, particularly useful for model-based workloads where documents like PDFs or images are processed through APIs.</li> <li><strong>Cross-language Support:</strong> JSON works seamlessly across programming languages. For instance, we could preprocess a document in Python, serialize the output to JSON, and read it into a JavaScript application.</li> <li><strong>Streaming Use Cases:</strong> JSON is also ideal for <strong>streaming applications</strong>, such as <strong>JSON-L</strong>, where large sets of documents are processed incrementally.</li> </ol> <h3 id="preprocessing-html-pages">Preprocessing HTML Pages</h3> <p><strong>HTML</strong> is one of the most common document types that need to be processed for LLM applications, especially when scraping web content. HTML is considered <strong>semi-structured</strong>, making it relatively easy to categorize content using <strong>tags</strong>. For example:</p> <ul> <li><strong>H1 tags</strong> typically indicate titles or headings.</li> <li><strong>Paragraph tags</strong> (<code class="language-plaintext highlighter-rouge">&lt;p&gt;</code>) are used to signify narrative text.</li> </ul> <p>However, content categorization isn‚Äôt always straightforward. While an <strong>H1</strong> tag reliably indicates a title, <strong>paragraph</strong> tags might include long descriptive text or short, capitalized strings that resemble headings. In such cases, <strong>natural language processing (NLP)</strong> techniques combined with HTML tags can help classify content more accurately.</p> <p>Tools like the <strong>unstructured open-source library</strong> simplify HTML preprocessing. Using functions like <code class="language-plaintext highlighter-rouge">partition_html()</code>, we can break down the document into core elements. These elements are then converted into a dictionary and serialized into <strong>JSON format</strong> for reuse. This structured format allows easy navigation of key sections, such as paragraphs or headers.</p> <h3 id="preprocessing-powerpoint-files">Preprocessing PowerPoint Files</h3> <p>In many business contexts, <strong>PowerPoint presentations</strong> are another common document type that LLM applications need to process. Like HTML, PowerPoint files are semi-structured, but the layout and formatting of elements differ significantly.</p> <p><strong>Titles</strong>, <strong>narrative text</strong>, and <strong>bullet points</strong> are formatted distinctively in PowerPoint files.</p> <p>Using the same <strong>normalization principles</strong> as with HTML, PowerPoint presentations can be broken down into core components to allow for <strong>uniform processing</strong> across different document types. This ensures that content extracted from PowerPoint files can be treated consistently, reducing complexity in further processing.</p> <h3 id="normalizing-pdfs-a-more-complex-challenge">Normalizing PDFs: A More Complex Challenge</h3> <p>Compared to HTML and PowerPoint, <strong>PDFs</strong> are often more complex because they rely heavily on <strong>visual cues</strong> to convey structure. This requires a different approach for normalization.</p> <p>In formats like HTML and PowerPoint, predefined structures such as <code class="language-plaintext highlighter-rouge">&lt;h1&gt;</code> tags or bullet points guide the differentiation of document elements. However, <strong>PDFs lack these predefined tags</strong>, so the document‚Äôs structure needs to be inferred through visual formatting. For example:</p> <ul> <li><strong>Bold and underlined text</strong> likely represents a title.</li> <li><strong>Long blocks of unformatted text</strong> typically indicate narrative content.</li> </ul> <p>These visual cues are essential for dividing content into categories such as titles, narrative text, or lists during preprocessing. Successfully normalizing PDFs requires carefully analyzing these formatting characteristics to ensure that the LLM can process them in the same way it handles other document types.</p> <h2 id="metadata-extraction-and-chunking">Metadata Extraction and Chunking</h2> <h3 id="what-is-metadata">What is Metadata?</h3> <p><strong>Metadata</strong> refers to additional information extracted during document preprocessing. It enriches the raw content and structures it for more efficient retrieval in <strong>Retrieval Augmented Generation (RAG)</strong> systems. Metadata exists at both the <strong>document level</strong> and <strong>element level</strong>:</p> <ol> <li><strong>Document-Level Metadata:</strong> Extracted directly from the document properties (e.g., file name, last modified date).</li> <li><strong>Element-Level Metadata:</strong> Inferred during preprocessing (e.g., element type like Title, hierarchical structure, page number).</li> </ol> <h3 id="semantic-search-for-llms">Semantic Search for LLMs</h3> <p><strong>Semantic search</strong> is an essential part of a RAG system, allowing it to retrieve documents from a <strong>vector database</strong> based on their similarity to a query. Here‚Äôs how it works:</p> <ol> <li><strong>Vector Database:</strong> Each document is embedded into a <strong>vector space</strong>, where documents with semantically similar content are placed close to each other.</li> <li><strong>Similarity Score:</strong> The system calculates a <strong>similarity score</strong> to determine how close the query vector is to the document vectors. Documents with closer vectors are more similar in content.</li> <li><strong>Retrieval:</strong> The documents with the highest similarity scores are retrieved and fed into the LLM to ensure that the content returned is relevant.</li> </ol> <h3 id="hybrid-search">Hybrid Search</h3> <p>While <strong>semantic search</strong> is effective for many tasks, it has some limitations, especially when additional contextual filters like time relevance or document structure are needed. <strong>Hybrid search</strong> combines <strong>semantic similarity</strong> with <strong>metadata filtering</strong> to refine the results more accurately.</p> <p><strong>Challenges with Semantic Search:</strong></p> <ol> <li> <p><strong>Too Many Matches:</strong></p> <p>Semantic search can return a large number of results, especially when documents cover the same topic. For instance, a search for ‚ÄúSuper Bowl‚Äù may return both pre-game and post-game content, even if we‚Äôre only interested in the results.</p> </li> <li> <p><strong>No Recency Filter:</strong></p> <p>By default, semantic search doesn‚Äôt prioritize more recent documents. This becomes problematic when we‚Äôre seeking the latest information.</p> </li> <li> <p><strong>Loss of Sectional Information:</strong></p> <p>Semantic search doesn‚Äôt consider document structure, such as titles or sections, which can lead to fragmented or less contextually relevant results.</p> </li> </ol> <p><strong>Benefits of Hybrid Search:</strong></p> <ol> <li> <p><strong>Combining Semantic and Structured Data:</strong></p> <p>Hybrid search improves upon semantic search by incorporating <strong>metadata filters</strong> (e.g., date, section). This enables more accurate results, as the system can restrict the search to more specific content.</p> </li> <li> <p><strong>Example Use Case:</strong></p> <p>For a query like ‚ÄúSuper Bowl results,‚Äù semantic search may return both pre-game predictions and post-game results. With <strong>hybrid search</strong>, we can filter out documents created before the game, ensuring that only post-game results are returned.</p> </li> </ol> <h3 id="chunking">Chunking</h3> <p><strong>Chunking</strong> involves breaking down large documents into smaller, manageable pieces, or <strong>chunks</strong>. This is necessary for two main reasons:</p> <ol> <li> <p><strong>LLM Context Window Limitations:</strong></p> <p>LLMs have a limited context window, meaning we can only pass a certain amount of text at a time. Chunking ensures that only relevant pieces of the document are sent to the LLM, preventing overload and improving response quality.</p> </li> <li> <p><strong>Cost Efficiency:</strong></p> <p>Larger context windows are more expensive to process. By chunking the content, we can save on inference costs by sending only the required information to the LLM.</p> </li> </ol> <p><strong>Chunking from Document Elements</strong></p> <p>There are multiple methods for chunking content, including <strong>chunking by characters</strong> or <strong>tokens</strong>. However, the most efficient approach is <strong>element-based chunking</strong>, which uses metadata to create more contextually accurate chunks.</p> <ol> <li> <p><strong>Traditional Chunking:</strong></p> <p>In traditional chunking, documents are split into even-sized chunks based on a threshold (e.g., character count). However, this method can fragment content across chunks. For example, a section discussing ‚ÄúOpen Domain Question Answering‚Äù might bleed into another chunk about ‚ÄúAbstractive Question Answering,‚Äù leading to less relevant results.</p> </li> <li> <p><strong>Element-Based Chunking:</strong></p> <p>Instead of cutting the document arbitrarily, element-based chunking breaks down documents into <strong>atomic elements</strong> (titles, narrative text, lists). Using <strong>logical break conditions</strong>, like creating a new chunk whenever a title is detected, keeps content that belongs together intact. For example, when chunking by title, all content related to ‚ÄúOpen Domain Question Answering‚Äù will remain in the same chunk, while irrelevant sections are excluded.</p> </li> </ol> <p>This leads to:</p> <ul> <li><strong>Better Search Results:</strong> More relevant content is retrieved from the vector database.</li> <li><strong>More Relevant Prompts:</strong> The LLM receives chunks that are contextually coherent.</li> <li><strong>Improved LLM Responses:</strong> Cleaner, more accurate responses to user queries.</li> </ul> <h2 id="preprocessing-pdfs-and-images">Preprocessing PDFs and Images</h2> <p>When working with unstructured document types like <strong>PDFs</strong> and <strong>images</strong>, which lack internal tags or structured data, advanced preprocessing techniques are required. These documents often require visual cues to infer structure, and this leads to two primary methodologies for <strong>document image analysis</strong>: <strong>Document Layout Models</strong> and <strong>Vision Transformers</strong>. Each approach has its own strengths and limitations when extracting and structuring data from unstructured documents.</p> <h3 id="document-layout-model">Document Layout Model</h3> <p>A <strong>Document Layout Model</strong> focuses on identifying and labeling the structure of a document by recognizing its components, such as titles, paragraphs, and lists, through <strong>bounding boxes</strong>. These bounding boxes enable the extraction of specific content elements from a document by identifying regions of interest like narrative text, tables, or headings.</p> <p><strong>How it Works:</strong></p> <ol> <li><strong>Bounding Box Identification:</strong> The model scans the document and draws bounding boxes around elements like titles, paragraphs, and tables, effectively isolating them.</li> <li><strong>Text Extraction:</strong> After labeling the boxes, text is extracted from within them. If the text is not embedded in the document (e.g., an image-based PDF), <strong>Optical Character Recognition (OCR)</strong> techniques are applied to extract the text. However, in some cases, such as PDFs with embedded text, the bounding box alone can be used to trace and extract content directly from the document.</li> </ol> <p>The <strong>YOLOx model</strong>, an object detection model, is a popular choice for document layout detection. This architecture excels in detecting and labeling document elements by drawing bounding boxes around them.</p> <h3 id="vision-transformers">Vision Transformers</h3> <p>Unlike Document Layout Models, <strong>Vision Transformers</strong> are designed to extract text and structured information from PDFs and images in a single step. These models bypass the need for bounding boxes and OCR, accepting an image as input and producing <strong>text</strong> or even <strong>structured JSON</strong> as output directly.</p> <h3 id="how-it-works">How it Works:</h3> <ol> <li><strong>Input:</strong> The <strong>Vision Transformer</strong> takes a document image as its input.</li> <li><strong>Processing:</strong> It processes the document and generates a valid output, often in a <strong>structured format</strong> like JSON.</li> <li><strong>Output:</strong> The output JSON contains both the text content and the categories of document elements (e.g., title, narrative text, list item).</li> </ol> <p>One common model architecture for vision transformers is the <strong>Donut Architecture</strong> (Document Understanding Transformer). This model can be trained to output structured data like JSON, making it particularly useful for documents like forms or invoices, where structured data is essential.</p> <h3 id="advantages-of-document-layout-models">Advantages of Document Layout Models</h3> <ol> <li><strong>Trained on Fixed Element Types:</strong> Document layout models are highly effective at recognizing specific, fixed elements such as titles, paragraphs, and tables. This specialization allows for high accuracy in identifying these predefined document structures.</li> <li><strong>Bounding Box Information:</strong> The bounding boxes provide clear, traceable locations of elements within a document. This is useful when the text can be extracted directly from the document without relying on OCR.</li> </ol> <h3 id="disadvantages-of-document-layout-models">Disadvantages of Document Layout Models</h3> <ol> <li><strong>Two Model Calls:</strong> Document layout models typically require two separate steps: first, the object detection model identifies and labels document elements, and second, an OCR model is used to extract the text if it‚Äôs not embedded in the document.</li> <li><strong>Limited Flexibility:</strong> These models are built to work with a <strong>fixed set of element types</strong>. Adapting the model to identify new elements or nonstandard document structures often requires additional retraining, making them less flexible.</li> </ol> <h3 id="advantages-of-vision-transformers">Advantages of Vision Transformers</h3> <ol> <li><strong>Flexibility for Nonstandard Document Types:</strong> Vision transformers excel in handling nonstandard or complex document types like forms, where key-value pairs or dynamic layouts may be present. Their flexibility allows them to process various document formats with ease.</li> <li><strong>Adaptable to New Ontologies:</strong> Vision transformers can easily adapt to new document structures or element types, allowing for more dynamic usage. Adding new element types is relatively straightforward through <strong>prompting</strong> without requiring model retraining.</li> </ol> <h3 id="disadvantages-of-vision-transformers">Disadvantages of Vision Transformers</h3> <ol> <li><strong>Potential for Hallucination:</strong> Vision transformers are generative models, which means they can sometimes produce content that wasn‚Äôt present in the original document. This issue, known as <strong>hallucination</strong>, can reduce accuracy and reliability in certain cases.</li> <li><strong>Higher Computational Cost:</strong> Vision transformers are computationally expensive compared to document layout models. They require more computing power and tend to process large documents more slowly, which may be a drawback for resource-constrained environments.</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Preprocessing unstructured data is crucial for large language models (LLMs) to function accurately and efficiently. By leveraging methods such as <strong>metadata extraction</strong>, <strong>hybrid search</strong>, <strong>chunking</strong>, and advanced models like <strong>Document Layout Models</strong> and <strong>Vision Transformers</strong>, we can convert diverse and complex documents into structured formats that LLMs can process effectively.</p> <p>Whether it‚Äôs normalizing different file types or extracting valuable information from PDFs and images, these preprocessing techniques form the foundation for scalable and high-performing LLM applications. Equipped with these methods, we can build systems that manage vast quantities of unstructured data while maintaining relevance and precision in their outputs.</p>]]></content><author><name></name></author><category term="llm"/><category term="prompt-engineering"/><category term="AI"/><category term="fine-tuning"/><category term="llm"/><category term="unstructured-data"/><category term="RAG"/><summary type="html"><![CDATA[Learn how to preprocess unstructured data for large language models (LLMs) using techniques like Retrieval Augmented Generation (RAG), metadata extraction, and advanced document analysis methods.]]></summary></entry><entry><title type="html">Fine-Tuning Large Language Models</title><link href="https://iampukar.github.io/blog/2024/fine-tuning-llm/" rel="alternate" type="text/html" title="Fine-Tuning Large Language Models"/><published>2024-09-23T11:46:00+00:00</published><updated>2024-09-23T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/fine-tuning-llm</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/fine-tuning-llm/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Fine-tuning large language models (LLMs) is a powerful strategy that allows us to personalize them for domain-specific tasks. While prompt engineering is effective for guiding models, fine-tuning takes it a step further by training them on our specific data, resulting in more accurate and tailored responses. This process enables us to fine-tune models to extract keywords, classify text, or adjust the tone of the model for particular tasks, thereby achieving even greater consistency.</p> <p>In this blog, we will explore <strong>the benefits of fine-tuning</strong>, compare it to prompt engineering, and demonstrate how we can leverage this approach for improved performance, privacy, and cost-efficiency.</p> <h2 id="why-fine-tune-large-language-models">Why Fine-Tune Large Language Models?</h2> <p>Fine-tuning allows us to customize general-purpose models like GPT-3 or GPT-4 for specific use cases. For instance, we can fine-tune GPT models to specialize in tasks like code completion, sentiment analysis, or any specific business application.</p> <p><strong>Key advantages of fine-tuning include</strong>:</p> <ul> <li><strong>Specialization</strong>: By training a model on task-specific data, we create a more sophisticated version that is finely tuned for our needs.</li> <li><strong>Data Efficiency</strong>: Fine-tuning allows us to provide much more data that can be typically fed into a prompt, enhancing the model‚Äôs learning and enabling it to become more specialized.</li> <li><strong>Consistency</strong>: Fine-tuning reduces variability in outputs, helping our model deliver more consistent results.</li> <li><strong>Hallucination Reduction</strong>: It helps mitigate hallucinations‚Äîwhere the model generates incorrect or irrelevant information‚Äîby aligning it more closely with the data we‚Äôve trained it on.</li> </ul> <h2 id="prompt-engineering-vs-fine-tuning">Prompt Engineering vs. Fine-Tuning</h2> <p>Both prompt engineering and fine-tuning have their roles. While prompt engineering is excellent for rapid prototyping and side projects, fine-tuning is ideal for <strong>enterprise-level solutions</strong> that require privacy, consistency, and large-scale performance.</p> <h3 id="prompt-engineering"><strong>Prompt Engineering</strong>:</h3> <p><strong>Pros</strong>:</p> <ul> <li>No need for pre-existing data to start.</li> <li>Low upfront and technical costs.</li> <li>We can connect data via retrieval systems like Retrieval-Augmented Generation (RAG).</li> </ul> <p><strong>Cons</strong>:</p> <ul> <li>Limited data can be processed in prompts.</li> <li>Models forget data between interactions.</li> <li>Higher risk of hallucinations and incorrect data retrieval.</li> </ul> <h3 id="fine-tuning"><strong>Fine-Tuning</strong>:</h3> <p><strong>Pros</strong>:</p> <ul> <li>Handles much larger datasets.</li> <li>Capable of learning and retaining more detailed information.</li> <li>Reduces incorrect information and enhances accuracy over time.</li> <li>Lower per-request cost for smaller, fine-tuned models.</li> </ul> <p><strong>Cons</strong>:</p> <ul> <li>Requires high-quality data and technical expertise.</li> <li>Upfront computational cost to train models.</li> <li>Needs maintenance and tuning over time.</li> </ul> <h2 id="the-benefits-of-fine-tuning-our-own-llm">The Benefits of Fine-Tuning Our Own LLM</h2> <p>Fine-tuning offers <strong>several key benefits</strong> across performance, privacy, cost, and reliability.</p> <h3 id="1-performance">1. <strong>Performance</strong>:</h3> <ul> <li>Reduces hallucinations, increasing accuracy.</li> <li>Ensures more consistent responses.</li> <li>Allows moderation of unwanted information or adjusts the model‚Äôs tone for specific contexts.</li> </ul> <h3 id="2-privacy">2. <strong>Privacy</strong>:</h3> <ul> <li>We can deploy fine-tuned models on-premise or in Virtual Private Clouds (VPC), minimizing exposure.</li> <li>Prevents data leakage and security breaches.</li> </ul> <h3 id="3-cost">3. <strong>Cost</strong>:</h3> <ul> <li>Lowers per-request costs after initial setup.</li> <li>Provides us with greater transparency and control over the system.</li> </ul> <h3 id="4-reliability">4. <strong>Reliability</strong>:</h3> <ul> <li>We gain control over uptime and ensure lower latency.</li> <li>Enhances moderation and quality assurance.</li> </ul> <h2 id="where-does-fine-tuning-fit-in">Where Does Fine-Tuning Fit In?</h2> <p>When LLMs are first trained, they start with zero knowledge about the world. Initially, they can only predict the next word in a sequence, based on large corpora scraped from the internet, often unlabeled.</p> <p>After pre-training, models acquire language comprehension, but they are limited by the general knowledge they learn during this process. Fine-tuning allows us to <strong>train these models further</strong> with specific, curated data, enabling them to become highly specialized for a task or domain.</p> <p>Fine-tuning can work with <strong>labeled or self-supervised data</strong>, and it requires much less data than initial training. It‚Äôs an essential tool for customizing AI for real-world applications.</p> <h2 id="what-fine-tuning-does">What Fine-Tuning Does</h2> <p>Fine-tuning can achieve both behavioral change and knowledge enhancement, depending on the data and tasks we apply it to.</p> <h3 id="1-behavioral-change">1. <strong>Behavioral Change</strong>:</h3> <p>Fine-tuning alters a model‚Äôs behavior to align it with specific tasks. It helps focus the model on specific capabilities, such as moderation or improved conversational skills.</p> <h3 id="2-knowledge-enhancement">2. <strong>Knowledge Enhancement</strong>:</h3> <p>Fine-tuning also increases the model‚Äôs understanding of specific, domain-relevant concepts. It corrects outdated or incorrect information, ensuring that our model stays current.</p> <h2 id="tasks-we-can-fine-tune-for">Tasks We Can Fine-Tune For</h2> <p>Fine-tuning is most effective for tasks that are <strong>well defined</strong> and <strong>specific</strong>. This includes:</p> <ul> <li><strong>Text extraction</strong>: For tasks like keyword extraction, topic identification, and routing.</li> <li><strong>Text expansion</strong>: For tasks such as generating chat responses, writing emails, or code generation.</li> </ul> <p>The clarity of the task is a key indicator of success. We must know exactly what constitutes a satisfactory versus bad output to achieve optimal fine-tuning results.</p> <h2 id="steps-for-first-time-fine-tuning">Steps for First-Time Fine-Tuning</h2> <p>For those fine-tuning for the first time, these steps can guide us:</p> <ol> <li>Use prompt engineering to identify tasks where the model can improve.</li> <li>Pick a task the LLM handles decently, but could do better at.</li> <li>Collect, let‚Äôs say, 1000 input-output pairs for the task.</li> <li>Fine-tune a smaller LLM on this dataset to detect measurable improvements.</li> </ol> <h3 id="instruction-fine-tuning">Instruction Fine-Tuning</h3> <p>Instruction fine-tuning teaches models to follow human instructions more naturally, making it ideal for building chatbots and similar interfaces.</p> <p>By preparing a dataset in Q&amp;A format, we can teach the model to generalize beyond the specific instructions in the fine-tuning dataset, making it more versatile in handling new queries.</p> <h2 id="data-preparation-for-fine-tuning">Data Preparation for Fine-Tuning</h2> <p>The quality of the data we use for fine-tuning is crucial. Here are a few tips for data preparation:</p> <ul> <li><strong>Better data</strong>: Use real-world, diverse, high-quality data.</li> <li><strong>Worse data</strong>: Avoid generated, homogeneous, or low-quality data.</li> </ul> <p>The steps for preparing data include:</p> <ul> <li>Collect instruction-response pairs.</li> <li>Concatenate these pairs (add a prompt template if possible).</li> <li>Tokenize the data‚Äîturn the text into numbers using the appropriate tokenizer for our model. Pad and truncate the data to ensure uniformity across inputs.</li> <li>Split the data into training and testing sets for proper evaluation.</li> </ul> <h2 id="general-training-process">General Training Process</h2> <p>To fine-tune a general AI model, we start by loading a pre-trained base model and feeding it the training data. During training, the model adjusts its internal parameters based on the loss calculated from the data. By fine-tuning the <strong>hyperparameters</strong> and using techniques like batch training and multiple epochs, we can significantly enhance the model‚Äôs performance.</p> <h2 id="evaluation-and-iteration">Evaluation and Iteration</h2> <p>Evaluating the performance of fine-tuned models can be challenging, especially for generative tasks. Metrics often fall short, and thus <strong>human evaluation becomes the most reliable approach</strong>.</p> <p>We can also use popular benchmarks such as:</p> <ul> <li><strong>ARC</strong> (grade-school-level questions)</li> <li><strong>HellaSwag</strong> (common-sense reasoning)</li> <li><strong>MMLU</strong> (multitask metric covering subjects like math and history)</li> <li><strong>TruthfulQA</strong> (measuring the model‚Äôs tendency to reproduce common online falsehoods).</li> </ul> <p>Evaluating our model against these benchmarks, or conducting A/B testing across models, can help us refine and improve performance over time.</p> <h2 id="error-analysis">Error Analysis</h2> <p>Understanding the base model‚Äôs behavior before fine-tuning is critical. By categorizing errors, we can make data-driven iterations to correct issues like verbosity or incorrect information.</p> <p>Examples of useful fixes include:</p> <ul> <li>Spell-checking</li> <li>Trimming overly verbose responses</li> <li>Reducing repetitive outputs</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Fine-tuning large language models provides a robust approach to tailoring our product to meet specific needs. It allows us to create more specialized, consistent, and reliable models while improving privacy, reducing hallucinations, and cutting down costs over time. By properly preparing data, focusing on well-defined tasks, and evaluating the model‚Äôs performance carefully, we can unlock the true potential of AI fine-tuning.</p> <p>Fine-tuning goes beyond basic prompt engineering by enabling us to adjust a model‚Äôs behavior and knowledge, making it suitable for domain-specific applications. Whether we‚Äôre aiming for better performance, tighter security, or reduced operational costs, fine-tuning is a vital approach that helps us fully leverage the capabilities of large language models in real-world applications.</p>]]></content><author><name></name></author><category term="llm"/><category term="prompt-engineering"/><category term="AI"/><category term="llm"/><category term="fine-tuning"/><category term="prompt-engineering"/><summary type="html"><![CDATA[A Comprehensive Guide To Fine-Tuning Large Language Models]]></summary></entry><entry><title type="html">Guide to LangChain for LLM Development</title><link href="https://iampukar.github.io/blog/2024/langchain-for-llm-development/" rel="alternate" type="text/html" title="Guide to LangChain for LLM Development"/><published>2024-09-22T11:46:00+00:00</published><updated>2024-09-22T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/langchain-for-llm-development</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/langchain-for-llm-development/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>As the demand for intelligent, language-driven applications increases, so does the need for robust, efficient frameworks that make developing such applications more manageable and scalable. This is where <strong>LangChain</strong> comes into the picture.</p> <p>LangChain is an <strong>open-source development framework for LLM applications</strong>, designed to provide a <strong>modular and flexible</strong> way of working with Large Language Models (LLMs). Whether we are using Python or JavaScript (TypeScript), LangChain offers a seamless experience by focusing on <strong>composition</strong> and <strong>modularity</strong>.</p> <p>In this blog, we‚Äôll explore how LangChain‚Äôs building blocks‚Äî<strong>components</strong>‚Äîcome together to power various use cases, from prompt creation to memory management, chains, and agents.</p> <h2 id="key-components-of-langchain">Key Components of LangChain</h2> <p>LangChain is structured around several key components that form the foundation of any LLM-powered application. Understanding these components helps us develop more complex and useful models. Let‚Äôs break them down.</p> <h3 id="1-models"><strong>1. Models</strong></h3> <p>At the heart of LangChain are <strong>language models</strong>. These models, powered by LLMs, act as the engine behind every LangChain application. When we pass data (like a prompt) to the model, it generates responses or makes decisions based on pre-trained knowledge.</p> <h3 id="2-prompts"><strong>2. Prompts</strong></h3> <p>Creating effective <strong>prompts</strong> is essential for any LLM application. LangChain simplifies this by allowing us to use <strong>prompt templates</strong>. A well-designed prompt can turn basic interactions into meaningful ones. As applications become more sophisticated, prompts tend to grow longer and more detailed.</p> <p>LangChain provides an easy way to structure prompts and ensures we always feed the model high-quality input.</p> <h3 id="3-parsers"><strong>3. Parsers</strong></h3> <p>Once the model generates its output, <strong>parsers</strong> come into play. A parser ensures that the output is formatted correctly, making it easier for us to integrate it into the workflow of the application. Parsers can take raw output and transform it into a usable format, simplifying everything from analysis to display.</p> <h3 id="4-memory"><strong>4. Memory</strong></h3> <p><strong>LLMs are stateless</strong>, meaning they don‚Äôt remember past interactions unless we explicitly provide them with context. For building applications like <strong>chatbots</strong>, we need the models to retain memory across conversations.</p> <p>LangChain provides memory components to help store and reuse context across interactions:</p> <ul> <li><strong>ConversationTokenBufferMemory</strong>: Retains a limited number of tokens to control memory size and costs.</li> <li><strong>ConversationBufferMemory</strong>: Stores a fixed number of past interactions.</li> <li><strong>ConversationSummaryBufferMemory</strong>: Summarizes past interactions, allowing us to keep essential context while optimizing memory.</li> </ul> <p>By leveraging these memory components, we can build intuitive and cost-effective chatbot systems where only necessary context is stored and reused.</p> <h3 id="5-chains"><strong>5. Chains</strong></h3> <p>In LangChain, <strong>chains</strong> let us sequence operations, creating workflows by linking individual steps. Here are the main types of chains:</p> <ul> <li><strong>LLMChain</strong>: A basic yet powerful chain where a prompt is passed through the model, and its output is returned.</li> <li><strong>SequentialChain</strong>: Combines multiple chains in sequence, where one chain‚Äôs output serves as input to the next. - <strong>SimpleSequentialChain</strong>: Takes in one input to generate one output. - <strong>SequentialChain</strong>: Supports multiple input forms to generate outputs.</li> <li><strong>MultiPromptChain</strong>: Useful when we need to route between different chains based on context.</li> <li><strong>LLMRouterChain</strong>: Routes different prompts to different chains, depending on the task.</li> </ul> <p>Chains help us develop dynamic applications that flow seamlessly from one step to the next.</p> <h3 id="6-agents"><strong>6. Agents</strong></h3> <p>LangChain allows us to use LLMs as <strong>agents</strong>‚Äîreasoning engines capable of processing new information, answering questions, and making decisions. Agents combine pre-learned knowledge with user-provided information to derive conclusions.</p> <p>LangChain provides agents for specific tasks, such as:</p> <ul> <li><strong>Wikipedia Agent</strong>: Retrieves relevant data from Wikipedia.</li> <li><strong>Calculator Agent</strong>: Performs calculations based on provided prompts.</li> </ul> <p>These agents unlock new possibilities for automating tasks and building advanced AI-powered workflows.</p> <h2 id="building-llm-applications-on-documents">Building LLM Applications on Documents</h2> <p>One of the challenges with LLMs is their ability to process only a few thousand words at a time. This limitation can make working with large documents tricky. LangChain offers a solution using <strong>embeddings</strong> and <strong>vector databases</strong>.</p> <h3 id="embeddings"><strong>Embeddings</strong></h3> <p>Embeddings convert text into numerical representations, capturing the meaning and context of the content. These vectorized forms allow us to compare similar chunks of text and improve the LLM‚Äôs understanding.</p> <h3 id="vector-databases"><strong>Vector Databases</strong></h3> <p>Once embeddings are created, we store them in a <strong>vector database</strong>. For large documents, LangChain breaks the text into smaller chunks, embedding each one for storage. When a query is received, the model retrieves the most relevant chunks from the vector database, providing a precise response.</p> <p>This process enables us to work efficiently with large documents while maintaining critical context.</p> <h2 id="evaluating-llms-with-langchain">Evaluating LLMs with LangChain</h2> <p>Evaluation is key to LLM development. LangChain provides tools for manual evaluation, helping us debug and test the model‚Äôs performance. Additionally, LLM-assisted evaluation allows us to use the model itself to test and evaluate outputs.</p> <h2 id="conclusion">Conclusion</h2> <p>LangChain is a game-changer for LLM developers, offering a flexible and modular framework that simplifies complex LLM development. Whether we‚Äôre building chatbots, reasoning agents, or document-based applications, LangChain enables us to create sophisticated, efficient solutions with ease.</p>]]></content><author><name></name></author><category term="LangChain"/><category term="AI"/><category term="LangChain"/><category term="llm"/><category term="prompt-engineering"/><summary type="html"><![CDATA[Learn LangChain for LLM development exploring its modular components, memory, embeddings, and chains to build advanced AI applications.]]></summary></entry><entry><title type="html">The Art of ChatGPT Prompt Engineering</title><link href="https://iampukar.github.io/blog/2024/art-of-chatgpt-prompt-engineering/" rel="alternate" type="text/html" title="The Art of ChatGPT Prompt Engineering"/><published>2024-09-18T11:46:00+00:00</published><updated>2024-09-18T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/art-of-chatgpt-prompt-engineering</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/art-of-chatgpt-prompt-engineering/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>As the digital world leans heavily into artificial intelligence (AI), one essential skill for developers is prompt engineering‚Äîparticularly when working with tools like ChatGPT. Crafting effective prompts for large language models (LLMs) can be the difference between a vague response and a highly accurate, task-specific result.</p> <p>In this blog, we‚Äôll delve into the heart of ChatGPT prompt engineering, covering everything from the basics of LLMs to practical guidelines for developing effective prompts. We‚Äôll also explore how to mitigate model limitations and iterate towards perfecting your prompts through experimentation. If you‚Äôre a developer aiming to streamline your AI interactions, this guide is your essential roadmap.</p> <h2 id="the-two-faces-of-large-language-models">The Two Faces of Large Language Models</h2> <p>Before diving into prompt strategies, it‚Äôs important to understand the backbone of ChatGPT: large language models (LLMs). LLMs come in two distinct types, and knowing how they operate can optimize the way we communicate with them.</p> <p><strong>Base LLMs</strong></p> <p>Base LLMs are trained to predict the next word in a sequence. While powerful, these models do not inherently understand our instructions‚Äîthey simply generate coherent text based on patterns they‚Äôve learned from vast amounts of training data.</p> <p><strong>Instruction-Tuned LLMs</strong></p> <p>Instruction-tuned models, through techniques like <strong>Reinforcement Learning with Human Feedback (RLHF)</strong>, are fine-tuned to follow human instructions. They excel in specific tasks by aligning better with human expectations, making them ideal when precise directives are critical.</p> <h2 id="the-cornerstone-of-prompt-engineering-clarity-and-specificity">The Cornerstone of Prompt Engineering: Clarity and Specificity</h2> <h3 id="1-clarity-over-brevity-be-clear-and-specific">1. Clarity over Brevity: Be clear and specific</h3> <p>In prompt engineering, clarity reigns supreme. Detailed instructions guide LLMs toward our desired outcome. Here‚Äôs how to do it right:</p> <ul> <li><strong>Use Delimiters to Prevent Misinterpretation</strong>: Separate different elements of the prompt using delimiters like triple quotes or angle brackets to help the model focus on specific sections.</li> <li><strong>Ask for Structured Outputs</strong>: Request specific formats like JSON or CSV to ensure the response is easily parsed and integrated into our workflow.</li> <li><strong>Guide the Model with Condition Checks</strong>: For tasks requiring certain prerequisites, we should ask the model to verify conditions before proceeding. This reduces errors in complex workflows.</li> <li><strong>Few-Shot Prompting for Precision</strong>: Offering examples helps the model learn our requirements quickly, ensuring more accurate responses.</li> </ul> <h3 id="2-give-the-model-time-to-think-encourage-deliberation">2. Give the Model Time to Think: Encourage Deliberation</h3> <p>For complex tasks, we should further instruct the model to take its time:</p> <ul> <li><strong>Step-by-Step Instructions</strong>: Break down the task into smaller, manageable steps rather than asking for an immediate solution.</li> <li><strong>Simulate a Reasoning Process</strong>: Instruct the model to explain its reasoning before reaching a conclusion, ensuring more thoughtful and accurate responses.</li> </ul> <h2 id="mitigating-model-limitations-addressing-hallucinations">Mitigating Model Limitations: Addressing Hallucinations</h2> <p>A common limitation of LLMs is hallucination‚Äîwhere the model generates plausible-sounding but incorrect information. In order to mitigate from this, we should instruct the model to gather relevant information before responding. For example:</p> <ul> <li><strong>Ask for Sources</strong>: Request citations or references to ensure factual accuracy.</li> <li><strong>Verify Data</strong>: Include a verification step in the prompt to check that the information aligns with reliable sources.</li> <li><strong>Refine Open-Ended Prompts</strong>: Narrow the scope of broad queries to focus on specific, verifiable data.</li> </ul> <h2 id="iterative-prompt-development-refining-our-approach">Iterative Prompt Development: Refining Our Approach</h2> <p>Effective prompt engineering is a continuous process. We can follow this iterative method to refine our prompts:</p> <h3 id="1-start-with-an-idea">1. Start with an Idea</h3> <p>Define the task clearly. Whether it‚Äôs summarizing data or generating content, clarity is key.</p> <h3 id="2-initial-implementation">2. Initial Implementation</h3> <p>Create a clear and structured prompt, using delimiters and examples where necessary.</p> <h3 id="3-experiment-and-analyze">3. Experiment and Analyze</h3> <p>Test the prompt and analyze the results. Did it meet our expectations? Were there inaccuracies?</p> <h3 id="4-iterate-with-refinement">4. Iterate with Refinement</h3> <p>Refine the prompt based on errors or unclear outputs. Clarify instructions, break down tasks, and test across different inputs.</p> <h2 id="conclusion">Conclusion</h2> <p>Mastering prompt engineering is both an art and a science. Developers writing clear, structured prompts, giving the model time to think, and refining through iterative development, can unlock the full potential of large language models. Understanding LLM types and mitigating common issues like hallucination ensures that we receive accurate, task-specific responses. With the right approach, ChatGPT like models can become a powerful tool in any developer‚Äôs toolkit.</p>]]></content><author><name></name></author><category term="chatgpt"/><category term="prompt-engineering"/><category term="AI"/><category term="chatgpt"/><category term="openai"/><category term="prompt-engineering"/><category term="llm"/><category term="AI"/><summary type="html"><![CDATA[A Comprehensive Guide To Prompt Engineering]]></summary></entry><entry><title type="html">The Rise of Liquid Restaking Tokens</title><link href="https://iampukar.github.io/blog/2024/liquid-restaking-tokens/" rel="alternate" type="text/html" title="The Rise of Liquid Restaking Tokens"/><published>2024-07-17T11:46:00+00:00</published><updated>2024-07-17T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/liquid-restaking-tokens</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/liquid-restaking-tokens/"><![CDATA[<h2 id="introduction-to-restaking">Introduction to Restaking</h2> <p>Blockchains face a significant challenge known as the cold start problem‚Äîthe difficulty of gaining initial momentum for a new network. This is particularly crucial in a distributed system where security and efficiency depend on widespread participation. Attracting users and securing the network can be an uphill battle without an initial base of validators and users.</p> <p>Enter restaking, an innovative approach that involves taking assets already staked to secure a blockchain network and repurposing them to secure additional external systems. This efficient use of resources improves the security of multiple platforms without requiring users to invest additional capital.</p> <h2 id="the-concept-of-liquid-restaking">The Concept of Liquid Restaking</h2> <p>Restaking is an approach to securing a blockchain network by allowing users to use their assets to protect external services. This method leverages existing staked assets, such as Ether (ETH), Liquid Staking Tokens (LSTs), or other tokens, and repurposes them to secure additional networks or services.</p> <h3 id="distinction-between-lst-and-lrt">Distinction between LST and LRT</h3> <p><strong>Liquid Staking Tokens (LSTs):</strong> LSTs represent staked assets in iquid form. These tokens are created when users stake their crypto assets (e.g., ETH) on a network and receive a token that represents their staked position. This allows them to continue participating in DeFi activities while still earning staking rewards. For example, staking ETH on a platform and receiving stETH, a token that represents the staked ETH, is an example of liquid staking.</p> <p><strong>Liquid Restaking Tokens (LRTs):</strong> LRTs are a subset of LSTs used specifically in the context of restaking. Restaking involves taking these already-staked assets and using them to secure additional external services or networks. Essentially, LRTs are LSTs that have an extra layer of utility, as they not only represent staked assets but also the additional security provided to other platforms through restaking.</p> <p>Since LRTs originate from liquid staking tokens and add an extra layer of functionality, every LRT starts as an LST. While all LRTs are liquid staking tokens, not every LST is used for restaking. LSTs may only provide liquidity and staking rewards, without the additional security role that LRTs fulfill.</p> <h3 id="restaking-process">Restaking Process</h3> <ol> <li> <p>Staking Assets: Initially, users stake or lock their crypto assets (e.g., ETH) in a primary blockchain network like Ethereum to help secure and validate the network. This process earns them rewards.</p> </li> <li> <p>Restaking: Users can then restake these already staked assets on platforms like EigenLayer. Their staked assets now also secure other external services, such as rollups, oracles, and bridges.</p> </li> <li> <p>Receiving LRTs: In return for restaking their assets, users receive LRTs, which represent their stake and can be used within various decentralized finance (DeFi) applications.</p> </li> </ol> <h3 id="types-of-restaking">Types of Restaking</h3> <ul> <li> <p>Native Restaking: This process involves setting an Ethereum validator‚Äôs withdrawal credentials to EigenLayer‚Äôs smart contracts, or EigenPod. To participate in native restaking, users must operate an Ethereum Validator node.</p> </li> <li> <p>Liquid Restaking: In this method, users deposit liquid staking tokens (LSTs) and EIGEN tokens into EigenLayer smart contracts. Unlike native restaking, this doesn‚Äôt require operating a validator node.</p> </li> </ul> <h2 id="the-role-of-eigenlayer">The Role of EigenLayer</h2> <p>EigenLayer is at the forefront of improving the efficiency of staked assets in the blockchain ecosystem. The primary goal of EigenLayer is to enhance the utility and security of staked Ether (ETH) by allowing it to support multiple systems simultaneously, not just the Ethereum network. This approach maximizes the economic value and security provided by staked assets.</p> <p>As of July 14, 2024, EigenLayer has secured over $15 billion in total value locked (TVL), making it a leader in the liquid restaking token (LRT) space. The broader liquid restaking category now includes over 20 different protocols, highlighting the growing adoption and importance of restaking mechanisms in the cryptocurrency landscape. New entrants like Renzo, Puffer Finance, and Kelp are contributing to the ongoing evolution and expansion of the liquid restaking ecosystem.</p> <p>EigenLayer contracts feature a 7-day withdrawal delay for LST tokens and native restaking to enhance the security of their ecosystem. The withdrawal window for the EIGEN token is 24 days to support future planned functionality unique to the token. By utilizing these restaking processes, users can maximize the utility and security of their staked assets, contributing to a more robust and secure blockchain ecosystem.</p> <h3 id="key-components-of-eigenlayer">Key Components of EigenLayer</h3> <ul> <li> <p>Actively Validated Services (AVS): AVS are external systems or applications that require their own distributed validation semantics for verification and rely on the security provided by staked assets. These services can include rollups, oracles, bridges, and other blockchain-based applications. EigenLayer extends the security benefits of staked ETH by integrating with AVS to these external services, thereby increasing their reliability and robustness.</p> </li> <li> <p>Operators: Operators are entities or individuals who run the AVS software and are responsible for validating the tasks and processes within it. They register in EigenLayer and allow ETH stakers to delegate their staked assets to them in the form of native ETH or LSTs, then opt in to provide various services (AVSs), enhancing the overall security and functionality of their networks.</p> <p>Becoming an operator in the EigenLayer ecosystem does not require a specific amount of delegated, restaked TVL. Any Ethereum address can serve as an operator. An address can function as both a restaker, engaging in either liquid or native restaking, and an operator simultaneously. An operator can participate in the EigenLayer network without having any restaked tokens.</p> </li> <li> <p>Restakers: Restakers are users who restake native ETH or Liquid Staking Tokens (LSTs) to the EigenLayer protocol. By doing so, they enhance the security and efficiency of multiple systems while earning rewards.</p> </li> </ul> <h2 id="benefits-of-liquid-restaking-tokens-lrts">Benefits of Liquid Restaking Tokens (LRTs)</h2> <p>Restaking allows Ethereum validators to join branched consensus networks of various applications. By shifting the responsibility of securing these applications to AVS operators, who are more secure both technically and organizationally, the risk of attacks is reduced, thus enhancing overall security. Without such architecture, decentralized platforms often rely on vulnerable third-party solutions like cross-chain bridges or oracles, which have been frequent targets in DeFi hacks and exploits.</p> <p>To become an AVS operator, an entity must first be an Ethereum validator, requiring a deposit of 32 ETH and specific technical expertise and equipment, which limits participation opportunities. EigenLayer mitigates this problem by allowing users to deposit assets into a pool while operators handle the technical work, similar to how Liquid Staking Derivatives (LSD) protocols function. LRT protocols further add liquid tokens to the chain, enabling greater capital utilization in DeFi operations. This encourages more restakers to join and more AVS operators to emerge, making these services more accessible.</p> <h3 id="enhancing-ethereum-staking">Enhancing Ethereum Staking</h3> <p>Liquid restaking protocols present a new opportunity to invigorate Ethereum‚Äôs staking ecosystem. Unlike traditional liquid staking protocols, which use user-deposited ETH solely for securing the proof-of-stake chain, liquid restaking protocols utilize funds to validate various AVSes, extending the security benefits of staked ETH to a broader range of blockchain applications. This democratizes the staking landscape and challenges the dominance of established liquid staking leaders.</p> <h3 id="simplifying-the-staking-process">Simplifying the Staking Process</h3> <p>Running a validator node involves complex tasks such as managing infrastructure, monitoring status, and addressing downtime, requiring technical expertise. Liquid restaking protocols simplify this process by managing these complexities behind the scenes. This makes it easier for individuals to participate in staking and benefit from the rewards without needing extensive technical knowledge.</p> <h3 id="demand-for-a-higher-eth-yield">Demand for a Higher ETH Yield</h3> <p>There is substantial demand for enhanced yields, and the LRT market is well-positioned to capitalize on this growing demand. By providing higher returns through diversified staking opportunities, LRTs attract users seeking better rewards within a reasonable risk spectrum.</p> <h3 id="gas-efficiency">Gas Efficiency</h3> <p>Restaking can validate various services, distributing enhanced rewards to AVS operators and indirectly to restakers. These rewards include ETH and other tokens. However, this process can become highly gas-intensive on the resource-limited Ethereum Layer 1. In contrast, LRTs can batch-collect rewards for the entire pool and distribute them efficiently among protocol holders, conserving user resources. This gas-efficient approach ensures that users receive maximum benefits with minimal transaction costs, making the system more sustainable and user-friendly.</p> <h2 id="risk-management-and-complexity">Risk Management and Complexity</h2> <p>Despite their benefits, LRTs are not without risks. Here are some key concerns:</p> <h3 id="systemic-risks">Systemic Risks</h3> <p>Any flaws in restaking mechanisms or underlying protocols could jeopardize Ethereum‚Äôs security and stability. Vulnerabilities in restaking processes can undermine the network‚Äôs integrity, causing potential downtimes or security breaches that affect the entire DeFi ecosystem reliant on Ethereum‚Äôs stability.</p> <h3 id="impact-on-ethereum-consensus">Impact on Ethereum Consensus</h3> <p>The creation of financial primitives like LRTs may affect Ethereum validators. Issues such as slashing nodes from third-party projects and expanding the validator set due to higher rewards could impact the Ethereum transaction finalization process.</p> <h3 id="decoupling-risks">Decoupling Risks</h3> <p>LRTs are susceptible to decoupling from the underlying value of staked assets, causing mispricing, liquidity challenges, and potential losses for LRT holders. Inaccurate valuation of LRTs can introduce additional risks when used as collateral in DeFi protocols, potentially triggering liquidations or compromising platform stability.</p> <h3 id="demand-and-sustainability">Demand and Sustainability</h3> <p>The long-term success of LRTs hinges on the actual demand for AVS operator services. If demand is insufficient, liquid restaking providers may struggle to integrate into the current blockchain infrastructure and create a sustainable economic model.</p> <h3 id="economic-implications">Economic Implications</h3> <p>The increasing appeal of passive income via LRT protocols could lead to a supply shock for native ETH, potentially causing a liquidity crisis. If the current trend continues, a sudden shift in supply dynamics could push investors away and destabilize the market.</p> <h2 id="conclusion">Conclusion</h2> <p>Liquid Restaking Tokens (LRTs) offer a promising solution for enhancing blockchain security and efficiency. They provide significant benefits by maximizing the utility of staked assets and democratizing the staking process. However, the associated risks‚Äîranging from systemic vulnerabilities to economic implications‚Äîmust be carefully managed. As the LRT ecosystem evolves, the sustainability and security of these protocols will depend on the actual demand for AVS operator services and the robustness of the underlying infrastructure. By understanding and addressing these risks, the potential of LRTs can be fully realized, contributing to a more secure and efficient blockchain environment.</p> <p><em>The <a href="https://www.linkedin.com/pulse/rise-liquid-restaking-pukar-acharya-9wpjf/">original article</a> was published on LinkedIn.</em></p>]]></content><author><name></name></author><category term="Web3"/><category term="liquid-restaking-tokens"/><category term="Web3"/><category term="security"/><summary type="html"><![CDATA[Learn how Liquid Restaking Tokens (LRTs) are revolutionizing blockchain security and efficiency by repurposing staked assets.]]></summary></entry><entry><title type="html">Understanding the UwU Lend Exploit</title><link href="https://iampukar.github.io/blog/2024/uwu-lend-exploit/" rel="alternate" type="text/html" title="Understanding the UwU Lend Exploit"/><published>2024-06-11T11:46:00+00:00</published><updated>2024-06-11T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/uwu-lend-exploit</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/uwu-lend-exploit/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>On June 10, 2024, UwU Lend was exploited across three different transactions on the Ethereum Mainnet due to a smart contract vulnerability, which resulted in a loss of over 5272 ETH, totaling approximately $23 million.</p> <h2 id="introduction-to-uwu-lend">Introduction to UwU Lend</h2> <p>UwU Lend is a decentralized, non-custodial liquidity market protocol where users can participate as depositors, borrowers, or LP stakers.</p> <h2 id="vulnerability-assessment">Vulnerability Assessment</h2> <p>The root cause of the exploit is due to the manipulation of the price oracle.</p> <h3 id="steps">Steps</h3> <p><strong>Step 1:</strong></p> <p>We attempt to analyze <a href="https://etherscan.io/tx/0xca1bbf3b320662c89232006f1ec6624b56242850f07e0f1dadbe4f69ba0d6ac3">one of the attack transaction</a> executed by <a href="https://etherscan.io/address/0x841ddf093f5188989fa1524e7b893de64b421f47">the exploiter</a>.</p> <p><strong>Step 2:</strong></p> <p>The vulnerable and exploited contract is actually a fork of AAVE v2, but the UwU protocol <a href="https://contract-diff.xyz/?address=0x05bfa9157e92690b179033ca2f6dd1e86b25ea4d&amp;chain=0">made some changes</a> to the fallback oracle.</p> <p><strong>Step 3:</strong></p> <p>The attacker initially took a flash loan of roughly $3.796 billion worth of assets from AAVE V3, AAVE V2, Uniswap V3, Balancer, Maker, Spark, and Morpho. Researchers within the DeFi security community cite that this is probably one of the largest ever borrowed amounts for a particular trade.</p> <p><strong>Step 4:</strong></p> <p>Approximately half of these borrowed assets <a href="https://x.com/CertiKAlert/status/1800195391214022672">were used to</a> create a leveraged position through recursive borrowing, in which the attacker held a huge amount of sUSDE debt.</p> <p><strong>Step 5:</strong></p> <p>The sUSDE price fetched through the <a href="https://etherscan.io/address/0xd252953818bdf8507643c237877020398fa4b2e8#code">sUSDePriceProviderBUniCatch contract</a> on UwU Lend uses the median of 11 different price sources, out of which five (FRAXUSDe, USDeUSDC, USDeDAI, USDecrvUSD, and GHOUSDe) could be easily manipulated using CurveFinance pools. This is possible because these oracles provide the price of the assets given the current state of the pool, such as their token balance, which can be easily manipulated.</p> <pre><code class="language-solidity">function getPrice() external view override returns (uint256) {
  (uint256[] memory prices, bool uniFail) = _getPrices(true);

  uint256 median = uniFail ? (prices[5] + prices[6]) / 2 : prices[5];

  require(median &gt; 0, "Median is zero");

  return FullMath.mulDiv(median, sUSDeScalingFactor, 1e3);
}
</code></pre> <pre><code class="language-solidity">function _getPrices(bool sorted) internal view returns (uint256[] memory, bool uniFail) {
  uint256[] memory prices = new uint256[](11);
  (prices[0], prices[1]) = _getUSDeFraxEMAInUSD();
  (prices[2], prices[3]) = _getUSDeUsdcEMAInUSD();
  (prices[4], prices[5]) = _getUSDeDaiEMAInUSD();
  (prices[6], prices[7]) = _getCrvUsdUSDeEMAInUSD();
  (prices[8], prices[9]) = _getUSDeGhoEMAInUSD();
  try UNI_V3_TWAP_USDT_ORACLE.getPrice() returns (uint256 price) {
    prices[10] = price;
  } catch {
    uniFail = true;
  }

  if (sorted) {
    _bubbleSort(prices);
  }

  return (prices, uniFail);
}
</code></pre> <p><strong>Step 6:</strong></p> <p>The other half of the earlier borrowed assets were used to manipulate the price of five oracles in reference so that the price of sUSDE was deemed more expensive than usual, which made the position insolvent. On the Curve Finance oracles, the price of sUSDE while borrowing was about 0.9, but the liquidation price stood at 1.03.</p> <p><strong>Step 7:</strong></p> <p>The attacker repeatedly liquidated the position to acquire uWETH, then reversed the manipulated asset price and repaid the flash loan to complete the attack and secure their profits.</p> <p><strong>Step 8:</strong></p> <p>These are the other two attack transactions in reference, <a href="https://etherscan.io/tx/0x242a0fb4fde9de0dc2fd42e8db743cbc197ffa2bf6a036ba0bba303df296408b">one of which</a> yielded the attacker approximately $7.2 million, while the attacker profited by roughly $7.6 million from <a href="https://etherscan.io/tx/0xb3f067618ce54bc26a960b660cfc28f9ea0315e2e9a1a855ede1508eb4017376">the other</a> attack transactions.</p> <p><strong>Step 9:</strong></p> <p>The stolen funds include assets in USDT, FRAX, bLUSD, and DAI, all of which were swapped for ETH and then split into two different EOAs, <a href="https://etherscan.io/address/0x48D7C1dd4214B41EDa3301BCA434348F8d1C5EB6">this</a> and <a href="https://etherscan.io/address/0x050c7E9c62Bf991841827F37745DDaDb563FEB70">this</a>. At the time of this writing, <a href="https://etherscan.io/address/0x48D7C1dd4214B41EDa3301BCA434348F8d1C5EB6">this address</a>, likely controlled by the attacker, has a hold of 1,282.9877 ETH, which is worth approximately $4,559,443.66. The <a href="https://etherscan.io/address/0x050c7E9c62Bf991841827F37745DDaDb563FEB70">other address</a> has a hold of 4,010 ETH, which is worth $14,242,406.</p> <p><strong>Step 10:</strong></p> <p>According to <a href="https://x.com/UwU_Lend/status/1800444603692376400">the team</a>, the total loss suffered by the protocol stands at $23 million, which includes 481.357407 WETH worth $1,704,005; 17.629563 WBTC worth $1,191,564; 499,254.38 bLUSD worth $592,614.95; 233,819.07 crvUSD worth $233,567.96; 1,394,055.37 sDAI worth $1,516,553.58; 25,354,902.10 CRV worth $9,381,313.80; 3,522,427.55 DAI worth $3,520,853.90; 4,224,277.30 USDT worth $4,223,114.99; and 486,455.22 sUSDe worth $525,371.64.</p> <h2 id="aftermath">Aftermath</h2> <p>The <a href="https://x.com/UwU_Lend/status/1800159455767843009">team acknowledged</a> the occurrence of the exploit and stated that they paused their protocol to contain the damage caused by the exploit. The attacker deposited much of the stolen assets into <a href="https://x.com/CurveFinance/status/1800269737563451705">Curve Finance-based Llama Lend Market</a>, only to later face a hard liquidation, and their position was completely liquidated.</p> <p>The team has further sent an <a href="https://etherscan.io/tx/0x31e5c9a15ce5697c9680cfdeaf5eda60379923d751d3b5eb685b28448d083f97">on-chain message</a> to the exploiter with hopes of retrieving 80% of the stolen funds in exchange for a 20% white hat bounty reward.</p> <h2 id="solution">Solution</h2> <p>To address the vulnerabilities exposed by the UwU Lend exploit, a comprehensive reassessment of the protocol‚Äôs price oracle implementation is essential. The use of a median of 11 price feeds, while initially seeming robust, proved insufficient due to the low liquidity and lack of price smoothing in half of these feeds. This allowed the attacker to manipulate the prices easily. A more resilient approach would involve the exclusion of low liquidity feeds or the integration of sophisticated smoothing and weighting mechanisms that enhance the oracle‚Äôs resistance to manipulation, thereby increasing the costs and efforts required for an attack.</p> <p>Curve Finance, <a href="https://resources.curve.fi/factory-pools/understanding-oracles/#profits-and-liquidity-balances">aware of the potential for manipulation in their pools</a>, explicitly advises against using them as standalone price oracles. They incorporate certain protections, like the update of the price oracle only once per block and an exponential moving average to dampen rapid changes. However, these measures alone were not enough to prevent the manipulation seen in the UwU Lend exploit. To build on these foundational measures, protocols should consider using more robust solutions, such as those provided by <a href="https://chain.link/education-hub/blockchain-vs-oracles">ChainLink</a>. ChainLink offers <a href="https://blog.chain.link/using-chainlink-oracles-to-securely-utilize-curve-lp-pools/">detailed methodologies</a> for securely integrating price feeds from liquidity pools, including those based on Curve, which can significantly reduce the risk of similar exploits.</p> <p>The incident also highlights the critical risks associated with <a href="https://github.com/YAcademy-Residents/defi-fork-bugs">DeFi protocol forks</a>. In this case, UwU Lend was a fork of AAVE v2 but had altered the fallback oracle without fully addressing the security implications of such changes. This oversight underscores the importance of thorough security audits and stress testing, especially when protocols make substantial modifications to their codebase. These audits should not only check for direct vulnerabilities but also evaluate the broader security architecture‚Äôs resilience against complex attack vectors like those involving multiple DeFi platforms.</p> <p>Furthermore, the massive scale of the flash loan used in this attack ‚Äî amounting to nearly $3.796 billion ‚Äî should have raised immediate red flags. Protocols can implement monitoring systems that trigger alerts or enforce limits when unusually large transactions or flash loans occur. Such systems could include dynamic restrictions that adjust based on typical transaction sizes and frequencies, providing an additional layer of security without hindering normal protocol operations.</p> <p><em>This article was <a href="https://medium.com/p/b32ea552f030">originally published</a> by Pukar Acharya for Neptune Mutual.</em></p>]]></content><author><name></name></author><category term="blockchain-hacks"/><category term="oracle-manipulation"/><category term="uwu-lend"/><summary type="html"><![CDATA[Learn how UwU Lend was exploited, which resulted in a loss of assets worth $23 million.]]></summary></entry><entry><title type="html">How Was Steam Swap Exploited?</title><link href="https://iampukar.github.io/blog/2024/steam-swap-exploit/" rel="alternate" type="text/html" title="How Was Steam Swap Exploited?"/><published>2024-06-08T11:46:00+00:00</published><updated>2024-06-08T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/steam-swap-exploit</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/steam-swap-exploit/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>On June 6, 2024, Steam Swap was exploited across two different transactions on the BNB chain due to a smart contract vulnerability, which resulted in a loss of assets worth approximately $105,000.</p> <h2 id="introduction-to-steam-swap">Introduction to Steam Swap</h2> <p>Steam Swap is a decentralized digital asset trading platform dedicated to bridging the global digital asset trading markets.</p> <h2 id="vulnerability-assessment">Vulnerability Assessment</h2> <p>The root cause of the exploit is due to the price manipulation of the underlying assets.</p> <h3 id="steps">Steps</h3> <p><strong>Step 1:</strong></p> <p>We attempt to analyze <a href="https://app.blocksec.com/explorer/tx/bsc/0x40f3bdd0a3a8d0476ae6aa2875dc2ec60b80812e2a394b67a88260df57c65522">one of the attack transactions</a> executed by <a href="https://bscscan.com/address/0xb5f1f0f3e9e72f94db32e8fcddde972ebfdc748e#tokentxns">the exploiter</a>.</p> <p><strong>Step 2:</strong></p> <p>The <a href="https://bscscan.com/address/0xb7d0a1adafa3e9e8d8e244c20b6277bee17a09b6#code">vulnerable MineSTM contract</a> has a <a href="https://bscscan.com/address/0xb7d0a1adafa3e9e8d8e244c20b6277bee17a09b6#code#F1#L633">sell function</a> that uses a reserve pair for liquidity calculation. Notably, this exploited contract was deployed roughly 16 hours before the incident took place.</p> <pre><code class="language-solidity">function sell(uint256 amount) external {
  eve_token_erc20.transferFrom(msg.sender, address(this), amount);
  (, uint256 r1, ) = inner_pair.getReserves();
  uint256 lpAmount = (amount * inner_pair.totalSupply()) / (2 * r1);
  uniswapV2Router.removeLiquidity(address(usdt_token_erc20), address(eve_token_erc20), lpAmount, 0, 0, msg.sender, block.timestamp);
}
</code></pre> <p><strong>Step 3:</strong></p> <p>The exploiter initially took a flash loan of 500,000 BSC-USD and used it to purchase roughly 2,740,041 STM tokens. The exploiter was able to manipulate this reserve balance by swapping a large amount of these tokens, and then ultimately called the above sell function to complete their attack.</p> <p><strong>Step 4:</strong></p> <p>The excess of the STM tokens were sold for profits worth approximately $91,670 before repaying the borrowed flash loan.</p> <p><strong>Step 5:</strong></p> <p><a href="https://bscscan.com/address/0x40a82dfdbf01630ea87a0372cf95fa8636fcad89">Another attacker</a>, likely a copycat of the original exploiter, executed <a href="https://bscscan.com/tx/0x849ed7f687cc2ebd1f7c4bed0849893e829a74f512b7f4a18aea39a3ef4d83b1">yet another attack transaction</a> to profit by roughly $13,892.</p> <h2 id="solution">Solution</h2> <p>The recent exploit of Steam Swap underscores the vital need for comprehensive security measures in smart contracts, especially given the sophisticated nature of this particular attack. Addressing the vulnerability observed in the reserve liquidity pair requires implementing stricter validations when processing sell functions in contracts. This could involve more robust calculations of liquidity based on accurate, real-time reserve checks, potentially integrating mechanisms like time-weighted averages to mitigate the risk of price manipulation.</p> <p>Preventing price manipulation is critical, and one effective strategy is employing ChainLink like Oracle services to provide reliable price feeds rather than solely depending on internal reserves, which can be easily distorted through market maneuvers such as flash loans.</p> <p>The fact that the contract was exploited shortly after its deployment raises concerns about its initial security measures. It suggests the possibility of an inside job or a deliberate act to deceive, leading to a potential rug-pull. These suspicions are compounded by the absence of a prior security audit, a standard practice that was surprisingly overlooked. Given the associations with reputable names like Uniswap, Binance, Metamask, and Andreessen Horowitz, the lack of initial auditing raises questions about the legitimacy of the operation and whether these partnerships were superficially used to gain trust.</p> <p>In <a href="https://x.com/SteamSwap_/status/1798832744816054592">response to the attack</a>, the team‚Äôs decision to conduct a post-incident audit, although a step in the right direction, comes too late for those affected by the exploit. The timeline for this audit, set at 7‚Äì10 business days, reflects urgency but also highlights the absence of proactive security measures. This incident serves as a stark reminder of the need for thorough vetting and continuous security assessments of smart contracts to avoid such vulnerabilities and maintain trust within the trading community. The ultimate determination of whether this was a high-level scam designed to lure traders and liquidity providers will depend on the outcomes of the ongoing investigations and audits.</p> <p><em>This article was <a href="https://medium.com/p/adb10ffe55bc">originally published</a> by Pukar Acharya for Neptune Mutual.</em></p>]]></content><author><name></name></author><category term="blockchain-hacks"/><category term="price-manipulation"/><category term="steam-swap"/><summary type="html"><![CDATA[Learn how Steam Swap was exploited, resulting in a loss of assets worth approximately $105,000.]]></summary></entry><entry><title type="html">Breaking down the 219M Gala Games Exploit</title><link href="https://iampukar.github.io/blog/2024/gala-games-exploit/" rel="alternate" type="text/html" title="Breaking down the 219M Gala Games Exploit"/><published>2024-05-21T11:46:00+00:00</published><updated>2024-05-21T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/gala-games-exploit</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/gala-games-exploit/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>On May 21, 2024, Gala Games was exploited on the Ethereum Mainnet, which resulted in the excess minting of 5 billion GALA tokens, which were worth approximately $219 million.</p> <h2 id="introduction-to-gala-games">Introduction to Gala Games</h2> <p>Gala Games is a web-based gaming platform that also extends its arm to the music and film industries.</p> <h2 id="vulnerability-assessment">Vulnerability Assessment</h2> <p>The root cause of the exploit remains unknown or uncertain. Was it a private key compromise, a case of <a href="https://x.com/Benefactor0101/status/1792698768166715776">private key misuse by an insider</a>, or perhaps even team involvement given the shady nature of some DeFi projects? This is all unclear.</p> <h3 id="incident-analysis">Incident Analysis</h3> <p>We attempt to analyze <a href="https://etherscan.io/tx/0xa6d90abe17d17743a9cecab84bcefb0fd0bbfa0c61bba60fd2f680b0a2f077fe">the attack transaction</a> executed by <a href="https://etherscan.io/address/0xe2ca471124b124831e231fb835778840ad100f97">the exploiter</a>.</p> <p>The minting of this massive number of tokens can be directly correlated with the compromise of the private keys of the deployer or the administrator wallet.</p> <p>The attacker then took to 0xProject to <a href="https://etherscan.io/advanced-filter?fadd=0xe2ca471124b124831e231fb835778840ad100f97&amp;tadd=%2C0xDef1C0ded9bec7F1a1670819833240f027b25EfF&amp;qt=1">dump these tokens</a> in chunks of 50 and 100 ETH.</p> <p>The exploiter was able to swap 599 million GALA tokens for roughly 5,913.20 ETH, which are worth approximately $21.8 million. At the time of this writing, the <a href="https://debank.com/profile/0xe2Ca471124b124831e231fb835778840Ad100F97">wallet of the exploiter</a> has a holding of approximately $206,737,478 worth of assets.</p> <p>The affected <a href="https://etherscan.io/address/0x8d92a6812b3da2346883f0631910c96cb9c5a5f9#code">Gala contract</a> has a <a href="https://etherscan.io/address/0x8d92a6812b3da2346883f0631910c96cb9c5a5f9#code#F1#L38"><code class="language-plaintext highlighter-rouge">notBlocklisted</code> modifier</a>, allowing the deployer to have privilege access to restrict an address for interacting with the protocol.</p> <pre><code class="language-solidity">modifier notBlocklisted(address _account) {
  require(!blocklisted[_account], "Account is blocklisted");
  _;
}
</code></pre> <p>Two hours and 16 minutes after the exploit, as viewed in <a href="https://etherscan.io/tx/0x15129c219a94e24d40541e622757973c0664338f117ff6c4b68d845854b167b9">this transaction</a>, the deployer has now added the address of the exploiter to the blocklist, effectively reducing the extent of the damage caused by the attack. Therefore, the actual loss of assets suffered by the protocol stands at $21.8 million. The excess of 4,401,236,462 GALA tokens will be burned by the team.</p> <h2 id="aftermath">Aftermath</h2> <p>The <a href="https://x.com/GoGalaGames/status/1792727587460104377">team acknowledged</a> the occurrence of the exploit and stated that the incident has been contained and the impacted wallet has been frozen. The exploit was an isolated incident, the cause of which has already been addressed, and the team is not closely working with law enforcement agencies to investigate the individuals behind the breach.</p> <p>Eric Schiermeyer, the CEO of Gala Games, took to X (formerly Twitter) to <a href="https://x.com/Benefactor0101/status/1792698768166715776">highlight the details</a> surrounding the incident. According to him, the Gala contract on the Ethereum Mainnet is guarded by a multisignature wallet, which was never compromised. This incident was the result of a loosely coupled internal control within the team.</p> <h2 id="solution">Solution</h2> <p>In response to the Gala Games exploit, it is imperative to address the underlying vulnerabilities and implement robust security measures to prevent future incidents. The exploit highlights the critical importance of safeguarding private keys, as their compromise can result in severe financial losses. Implementing secure storage solutions is paramount, with hardware wallets being a recommended option for cold storage. These wallets keep the majority of assets offline, reducing exposure to online threats. For operational liquidity, a minimal amount of assets should be kept in hot wallets, protected by stringent security protocols.</p> <p>Multi-signature wallets provide an additional layer of security by requiring multiple parties to authorize transactions. This significantly mitigates the risk of unauthorized access due to compromised keys. Regular security audits and vulnerability assessments are essential to identify potential security gaps and ensure that private key management protocols remain secure and up-to-date.</p> <p>The Gala Games team must also focus on continuous education and vigilance against phishing, social engineering, and malware threats. Regular security training for team members on the latest threats and secure communication practices can substantially reduce the risk of such attacks. Keeping software, including wallets and security tools, updated is crucial for defending against known vulnerabilities.</p> <p>Despite the existence of blacklist functionality in the Gala Games protocol, the team took over two hours to restrict the attacker‚Äôs access, by which time significant damage had already been done, resulting in losses of over $21 million. However, this functionality did help prevent a larger scale of damage. To enhance the protocol‚Äôs response time, the team should implement automated monitoring and alert systems to detect and respond to suspicious activities more swiftly.</p> <p><em>This article was <a href="https://medium.com/p/1884b5d4e5b6">originally published</a> by Pukar Acharya for Neptune Mutual.</em></p>]]></content><author><name></name></author><category term="blockchain-hacks"/><category term="private-key-compromise"/><category term="gala-games"/><summary type="html"><![CDATA[Learn how Gala Games was exploited, resulting in a loss of assets worth $219 million.]]></summary></entry><entry><title type="html">Analysis of the Tsuru Exploit</title><link href="https://iampukar.github.io/blog/2024/tsuru-exploit/" rel="alternate" type="text/html" title="Analysis of the Tsuru Exploit"/><published>2024-05-13T11:46:00+00:00</published><updated>2024-05-13T11:46:00+00:00</updated><id>https://iampukar.github.io/blog/2024/tsuru-exploit</id><content type="html" xml:base="https://iampukar.github.io/blog/2024/tsuru-exploit/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>On May 10, 2024, Tsuru was exploited on the Base chain due to a smart contract vulnerability, which resulted in a loss of 137.78 ETH, which was worth approximately $410,000.</p> <h2 id="introduction-to-tsuru">Introduction to Tsuru</h2> <p>Tsuru is the original character created by a supposed Japanese illustrator, Tsurushima Tatsumi.</p> <h2 id="vulnerability-assessment">Vulnerability Assessment</h2> <p>The root cause of the exploit is a lack of regulated access control.</p> <h3 id="steps">Steps</h3> <p><strong>Step 1:</strong></p> <p>We attempt to analyse <a href="https://app.blocksec.com/explorer/tx/base/0xe63a8df8759f41937432cd34c590d85af61b3343cf438796c6ed2c8f5b906f62">the attack transaction</a> executed by <a href="https://basescan.org/address/0x7a5eb99c993f4c075c222f9327abc7426cfae386">the exploiter</a>.</p> <p><strong>Step 2:</strong></p> <p>The exploited <a href="https://basescan.org/address/0x75ac62ea5d058a7f88f0c3a5f8f73195277c93da#code">TSURUWrapper contract</a> had a vulnerable <a href="https://basescan.org/address/0x75ac62ea5d058a7f88f0c3a5f8f73195277c93da#code#L1848">onERC1155Received function</a> that lacked proper authentication and enough access control.</p> <pre><code class="language-solidity">function onERC1155Received(address, address from, uint256 id, uint256 amount, bytes calldata) external override nonReentrant returns (bytes4) {
  require(id == tokenID, "Token ID does not match");

  if (msg.sender == address(erc1155Contract)) {
    return this.onERC1155Received.selector;
  }

  _safeMint(from, amount * ERC1155_RATIO); // Adjust minting based on the ERC1155_RATIO
  return this.onERC1155Received.selector;
}
</code></pre> <p><strong>Step 3:</strong></p> <p>As long as the id or tokenID parameter in this function corresponds to the ID of the project, it would allow anyone to mint the TSURU token and later swap it for ETH in the Uniswap Liquidity pool.</p> <p><strong>Step 4:</strong></p> <p>This exploiter on the Base chain <a href="https://basescan.org/tx/0xfe091a2d175f488bd366d3a84e9c37a622d789bf4539defacfc5f2a08169e2ca">swapped</a> the <a href="https://basescan.org/tx/0x1f51bdcc52a44e81360597aeb77050c5049df202692a7121e352fdf7bde6283d">stolen 167 million Tsuru</a> for 137.78 ETH, which were worth approximately $410,000, and <a href="https://www.basescan.org/tx/0x1f51bdcc52a44e81360597aeb77050c5049df202692a7121e352fdf7bde6283d">then bridged</a> it to <a href="https://etherscan.io/address/0x5E209c84E8632c011B7B5209dda3f7e50409C446">this address</a> on the Ethereum Mainnet. This address on the Ethereum Mainnet also <a href="https://etherscan.io/tx/0x1a02f2843c45bed3e3c126e0778346a4d30b4e958ebed31d2e463a9be6558e05">received 40.95 ETH</a> worth of assets from <a href="https://twitter.com/PerpyFinance/status/1787385427403395562">an old incident</a> that traces back to the incident involving <a href="https://twitter.com/MetaSleuth/status/1788966866511438268">Perpy Finance</a>.</p> <p><strong>Step 5:</strong></p> <p>The project was hacked <a href="https://basescan.org/tx/0x0812740dc17439c08876ab94926404f4b13d0457d4f17ef355ba8ad02c18f8c0">roughly two hours</a> after its deployment, and at the time of this writing, the <a href="https://debank.com/profile/0x5e209c84e8632c011b7b5209dda3f7e50409c446">address of the exploiter on the Ethereum Mainnet</a> has a hold of 179.68 ETH, which is worth approximately $516,129.</p> <h2 id="aftermath">Aftermath</h2> <p>The <a href="https://twitter.com/TsuruBase/status/1788941543514223046">team acknowledged</a> the occurrence of the exploit and has also shared a detailed post-mortem report regarding the incident.</p> <h2 id="solution">Solution</h2> <p>To address the vulnerabilities exploited in the Tsuru smart contract, a multi-faceted approach focused on strengthening security protocols, improving contract design, and enhancing monitoring systems is essential. Firstly, introducing rigorous access control measures in smart contracts is critical. For the Tsuru case, specifically ensuring that the <code class="language-plaintext highlighter-rouge">onERC1155Received</code> function includes authentication checks to verify not only the contract addresses interacting but also the roles of the interacting accounts would prevent unauthorized access. This could be implemented by incorporating a modifier that checks if the caller is from a list of addresses pre-approved by the contract owner or holds a specific role defined within the contract‚Äôs governance framework.</p> <p>Additionally, enhancing the contract‚Äôs logic to handle unexpected inputs more safely is necessary. For instance, implementing checks that validate the state changes within the contract before and after executing critical functions could help. This ensures that even if an unauthorized party were to call a function, they would not be able to cause state changes that could lead to asset theft or other undesired outcomes.</p> <p>Improving the testing and deployment procedures is another crucial step. Before deployment, the contract should undergo thorough testing, including both automated tests and manual peer reviews. Utilizing testnets to simulate real-world usage and attacks can help uncover vulnerabilities that might not be evident in isolated tests. Furthermore, engaging external auditors to conduct security audits and offering bug bounty programs can incentivize the discovery and resolution of security flaws before they can be exploited maliciously.</p> <p><em>This article was <a href="https://medium.com/p/630816199ff4">originally published</a> by Pukar Acharya elsewhere.</em></p>]]></content><author><name></name></author><category term="blockchain-hacks"/><category term="access-control"/><category term="tsuru"/><summary type="html"><![CDATA[Learn how Tsuru was exploited, resulting in a loss of 137.78 ETH which is worth $410,000.]]></summary></entry></feed>