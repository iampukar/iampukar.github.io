<!doctype html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fine-Tuning Large Language Models | Pukar Acharya </title> <meta name="author" content="Pukar Acharya"> <meta name="description" content="A Comprehensive Guide To Fine-Tuning Large Language Models"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>" > <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iampukar.github.io/blog/2024/fine-tuning-llm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pukar</span> Acharya </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-Tuning Large Language Models</h1> <p class="post-meta"> Created in September 23, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> &nbsp; &middot; &nbsp; <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a> &nbsp; <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a> &nbsp; <a href="/blog/tag/prompt-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> prompt-engineering</a> &nbsp; &middot; &nbsp; <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a> &nbsp; <a href="/blog/category/prompt-engineering"> <i class="fa-solid fa-tag fa-sm"></i> prompt-engineering</a> &nbsp; <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>Fine-tuning large language models (LLMs) is a powerful strategy that allows us to personalize them for domain-specific tasks. While prompt engineering is effective for guiding models, fine-tuning takes it a step further by training them on our specific data, resulting in more accurate and tailored responses. This process enables us to fine-tune models to extract keywords, classify text, or adjust the tone of the model for particular tasks, thereby achieving even greater consistency.</p> <p>In this blog, we will explore <strong>the benefits of fine-tuning</strong>, compare it to prompt engineering, and demonstrate how we can leverage this approach for improved performance, privacy, and cost-efficiency.</p> <h2 id="why-fine-tune-large-language-models">Why Fine-Tune Large Language Models?</h2> <p>Fine-tuning allows us to customize general-purpose models like GPT-3 or GPT-4 for specific use cases. For instance, we can fine-tune GPT models to specialize in tasks like code completion, sentiment analysis, or any specific business application.</p> <p><strong>Key advantages of fine-tuning include</strong>:</p> <ul> <li><strong>Specialization</strong>: By training a model on task-specific data, we create a more sophisticated version that is finely tuned for our needs.</li> <li><strong>Data Efficiency</strong>: Fine-tuning allows us to provide much more data that can be typically fed into a prompt, enhancing the model’s learning and enabling it to become more specialized.</li> <li><strong>Consistency</strong>: Fine-tuning reduces variability in outputs, helping our model deliver more consistent results.</li> <li><strong>Hallucination Reduction</strong>: It helps mitigate hallucinations—where the model generates incorrect or irrelevant information—by aligning it more closely with the data we’ve trained it on.</li> </ul> <h2 id="prompt-engineering-vs-fine-tuning">Prompt Engineering vs. Fine-Tuning</h2> <p>Both prompt engineering and fine-tuning have their roles. While prompt engineering is excellent for rapid prototyping and side projects, fine-tuning is ideal for <strong>enterprise-level solutions</strong> that require privacy, consistency, and large-scale performance.</p> <h3 id="prompt-engineering"><strong>Prompt Engineering</strong>:</h3> <p><strong>Pros</strong>:</p> <ul> <li>No need for pre-existing data to start.</li> <li>Low upfront and technical costs.</li> <li>We can connect data via retrieval systems like Retrieval-Augmented Generation (RAG).</li> </ul> <p><strong>Cons</strong>:</p> <ul> <li>Limited data can be processed in prompts.</li> <li>Models forget data between interactions.</li> <li>Higher risk of hallucinations and incorrect data retrieval.</li> </ul> <h3 id="fine-tuning"><strong>Fine-Tuning</strong>:</h3> <p><strong>Pros</strong>:</p> <ul> <li>Handles much larger datasets.</li> <li>Capable of learning and retaining more detailed information.</li> <li>Reduces incorrect information and enhances accuracy over time.</li> <li>Lower per-request cost for smaller, fine-tuned models.</li> </ul> <p><strong>Cons</strong>:</p> <ul> <li>Requires high-quality data and technical expertise.</li> <li>Upfront computational cost to train models.</li> <li>Needs maintenance and tuning over time.</li> </ul> <h2 id="the-benefits-of-fine-tuning-our-own-llm">The Benefits of Fine-Tuning Our Own LLM</h2> <p>Fine-tuning offers <strong>several key benefits</strong> across performance, privacy, cost, and reliability.</p> <h3 id="1-performance">1. <strong>Performance</strong>:</h3> <ul> <li>Reduces hallucinations, increasing accuracy.</li> <li>Ensures more consistent responses.</li> <li>Allows moderation of unwanted information or adjusts the model’s tone for specific contexts.</li> </ul> <h3 id="2-privacy">2. <strong>Privacy</strong>:</h3> <ul> <li>We can deploy fine-tuned models on-premise or in Virtual Private Clouds (VPC), minimizing exposure.</li> <li>Prevents data leakage and security breaches.</li> </ul> <h3 id="3-cost">3. <strong>Cost</strong>:</h3> <ul> <li>Lowers per-request costs after initial setup.</li> <li>Provides us with greater transparency and control over the system.</li> </ul> <h3 id="4-reliability">4. <strong>Reliability</strong>:</h3> <ul> <li>We gain control over uptime and ensure lower latency.</li> <li>Enhances moderation and quality assurance.</li> </ul> <h2 id="where-does-fine-tuning-fit-in">Where Does Fine-Tuning Fit In?</h2> <p>When LLMs are first trained, they start with zero knowledge about the world. Initially, they can only predict the next word in a sequence, based on large corpora scraped from the internet, often unlabeled.</p> <p>After pre-training, models acquire language comprehension, but they are limited by the general knowledge they learn during this process. Fine-tuning allows us to <strong>train these models further</strong> with specific, curated data, enabling them to become highly specialized for a task or domain.</p> <p>Fine-tuning can work with <strong>labeled or self-supervised data</strong>, and it requires much less data than initial training. It’s an essential tool for customizing AI for real-world applications.</p> <h2 id="what-fine-tuning-does">What Fine-Tuning Does</h2> <p>Fine-tuning can achieve both behavioral change and knowledge enhancement, depending on the data and tasks we apply it to.</p> <h3 id="1-behavioral-change">1. <strong>Behavioral Change</strong>:</h3> <p>Fine-tuning alters a model’s behavior to align it with specific tasks. It helps focus the model on specific capabilities, such as moderation or improved conversational skills.</p> <h3 id="2-knowledge-enhancement">2. <strong>Knowledge Enhancement</strong>:</h3> <p>Fine-tuning also increases the model’s understanding of specific, domain-relevant concepts. It corrects outdated or incorrect information, ensuring that our model stays current.</p> <h2 id="tasks-we-can-fine-tune-for">Tasks We Can Fine-Tune For</h2> <p>Fine-tuning is most effective for tasks that are <strong>well defined</strong> and <strong>specific</strong>. This includes:</p> <ul> <li><strong>Text extraction</strong>: For tasks like keyword extraction, topic identification, and routing.</li> <li><strong>Text expansion</strong>: For tasks such as generating chat responses, writing emails, or code generation.</li> </ul> <p>The clarity of the task is a key indicator of success. We must know exactly what constitutes a satisfactory versus bad output to achieve optimal fine-tuning results.</p> <h2 id="steps-for-first-time-fine-tuning">Steps for First-Time Fine-Tuning</h2> <p>For those fine-tuning for the first time, these steps can guide us:</p> <ol> <li>Use prompt engineering to identify tasks where the model can improve.</li> <li>Pick a task the LLM handles decently, but could do better at.</li> <li>Collect, let’s say, 1000 input-output pairs for the task.</li> <li>Fine-tune a smaller LLM on this dataset to detect measurable improvements.</li> </ol> <h3 id="instruction-fine-tuning">Instruction Fine-Tuning</h3> <p>Instruction fine-tuning teaches models to follow human instructions more naturally, making it ideal for building chatbots and similar interfaces.</p> <p>By preparing a dataset in Q&amp;A format, we can teach the model to generalize beyond the specific instructions in the fine-tuning dataset, making it more versatile in handling new queries.</p> <h2 id="data-preparation-for-fine-tuning">Data Preparation for Fine-Tuning</h2> <p>The quality of the data we use for fine-tuning is crucial. Here are a few tips for data preparation:</p> <ul> <li><strong>Better data</strong>: Use real-world, diverse, high-quality data.</li> <li><strong>Worse data</strong>: Avoid generated, homogeneous, or low-quality data.</li> </ul> <p>The steps for preparing data include:</p> <ul> <li>Collect instruction-response pairs.</li> <li>Concatenate these pairs (add a prompt template if possible).</li> <li>Tokenize the data—turn the text into numbers using the appropriate tokenizer for our model. Pad and truncate the data to ensure uniformity across inputs.</li> <li>Split the data into training and testing sets for proper evaluation.</li> </ul> <h2 id="general-training-process">General Training Process</h2> <p>To fine-tune a general AI model, we start by loading a pre-trained base model and feeding it the training data. During training, the model adjusts its internal parameters based on the loss calculated from the data. By fine-tuning the <strong>hyperparameters</strong> and using techniques like batch training and multiple epochs, we can significantly enhance the model’s performance.</p> <h2 id="evaluation-and-iteration">Evaluation and Iteration</h2> <p>Evaluating the performance of fine-tuned models can be challenging, especially for generative tasks. Metrics often fall short, and thus <strong>human evaluation becomes the most reliable approach</strong>.</p> <p>We can also use popular benchmarks such as:</p> <ul> <li><strong>ARC</strong> (grade-school-level questions)</li> <li><strong>HellaSwag</strong> (common-sense reasoning)</li> <li><strong>MMLU</strong> (multitask metric covering subjects like math and history)</li> <li><strong>TruthfulQA</strong> (measuring the model’s tendency to reproduce common online falsehoods).</li> </ul> <p>Evaluating our model against these benchmarks, or conducting A/B testing across models, can help us refine and improve performance over time.</p> <h2 id="error-analysis">Error Analysis</h2> <p>Understanding the base model’s behavior before fine-tuning is critical. By categorizing errors, we can make data-driven iterations to correct issues like verbosity or incorrect information.</p> <p>Examples of useful fixes include:</p> <ul> <li>Spell-checking</li> <li>Trimming overly verbose responses</li> <li>Reducing repetitive outputs</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Fine-tuning large language models provides a robust approach to tailoring our product to meet specific needs. It allows us to create more specialized, consistent, and reliable models while improving privacy, reducing hallucinations, and cutting down costs over time. By properly preparing data, focusing on well-defined tasks, and evaluating the model’s performance carefully, we can unlock the true potential of AI fine-tuning.</p> <p>Fine-tuning goes beyond basic prompt engineering by enabling us to adjust a model’s behavior and knowledge, making it suitable for domain-specific applications. Whether we’re aiming for better performance, tighter security, or reduced operational costs, fine-tuning is a vital approach that helps us fully leverage the capabilities of large language models in real-world applications.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/preprocessing-unstructured-data-for-llm-applications/">Preprocessing Unstructured Data for LLM Applications</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/langchain-for-llm-development/">Guide to LangChain for LLM Development</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/art-of-chatgpt-prompt-engineering/">The Art of ChatGPT Prompt Engineering</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/liquid-restaking-tokens/">The Rise of Liquid Restaking Tokens</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/uwu-lend-exploit/">Understanding the UwU Lend Exploit</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> &copy; Copyright 2024 Pukar Acharya. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hideBreadcrumbs noAutoLoadMdIcons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"These are details of some of the publicly available GitHub based repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-preprocessing-unstructured-data-for-llm-applications",title:"Preprocessing Unstructured Data for LLM Applications",description:"Learn how to preprocess unstructured data for large language models (LLMs) using techniques like Retrieval Augmented Generation (RAG), metadata extraction, and advanced document analysis methods.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preprocessing-unstructured-data-for-llm-applications/"}},{id:"post-fine-tuning-large-language-models",title:"Fine-Tuning Large Language Models",description:"A Comprehensive Guide To Fine-Tuning Large Language Models",section:"Posts",handler:()=>{window.location.href="/blog/2024/fine-tuning-llm/"}},{id:"post-guide-to-langchain-for-llm-development",title:"Guide to LangChain for LLM Development",description:"Learn LangChain for LLM development exploring its modular components, memory, embeddings, and chains to build advanced AI applications.",section:"Posts",handler:()=>{window.location.href="/blog/2024/langchain-for-llm-development/"}},{id:"post-the-art-of-chatgpt-prompt-engineering",title:"The Art of ChatGPT Prompt Engineering",description:"A Comprehensive Guide To Prompt Engineering",section:"Posts",handler:()=>{window.location.href="/blog/2024/art-of-chatgpt-prompt-engineering/"}},{id:"post-the-rise-of-liquid-restaking-tokens",title:"The Rise of Liquid Restaking Tokens",description:"Learn how Liquid Restaking Tokens (LRTs) are revolutionizing blockchain security and efficiency by repurposing staked assets.",section:"Posts",handler:()=>{window.location.href="/blog/2024/liquid-restaking-tokens/"}},{id:"post-understanding-the-uwu-lend-exploit",title:"Understanding the UwU Lend Exploit",description:"Learn how UwU Lend was exploited, which resulted in a loss of assets worth $23 million.",section:"Posts",handler:()=>{window.location.href="/blog/2024/uwu-lend-exploit/"}},{id:"post-how-was-steam-swap-exploited",title:"How Was Steam Swap Exploited?",description:"Learn how Steam Swap was exploited, resulting in a loss of assets worth approximately $105,000.",section:"Posts",handler:()=>{window.location.href="/blog/2024/steam-swap-exploit/"}},{id:"post-breaking-down-the-219m-gala-games-exploit",title:"Breaking down the 219M Gala Games Exploit",description:"Learn how Gala Games was exploited, resulting in a loss of assets worth $219 million.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gala-games-exploit/"}},{id:"post-analysis-of-the-tsuru-exploit",title:"Analysis of the Tsuru Exploit",description:"Learn how Tsuru was exploited, resulting in a loss of 137.78 ETH which is worth $410,000.",section:"Posts",handler:()=>{window.location.href="/blog/2024/tsuru-exploit/"}},{id:"post-how-was-galaxy-fox-token-exploited",title:"How Was Galaxy Fox Token Exploited?",description:"Learn how Galaxy Fox Token was exploited, resulting in a loss of assets worth $330,000.",section:"Posts",handler:()=>{window.location.href="/blog/2024/galaxy-fox-token-exploit/"}},{id:"post-how-was-ngfs-token-exploited",title:"How Was NGFS Token Exploited?",description:"Learn how NGFS Token was exploited, resulting in a loss of assets worth approximately $191,000.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ngfs-token-exploit/"}},{id:"post-how-was-sumer-money-exploited",title:"How Was Sumer Money Exploited?",description:"Learn how Sumer Money was exploited, resulting in a loss of assets worth $310,000.",section:"Posts",handler:()=>{window.location.href="/blog/2024/sumer-money-exploit/"}},{id:"post-analysis-of-the-miner-exploit",title:"Analysis of the Miner Exploit",description:"Learn how Miner was exploited, which resulted in a loss of assets worth 168.8 ETH.",section:"Posts",handler:()=>{window.location.href="/blog/2024/miner-exploit/"}},{id:"post-how-was-concentric-finance-exploited",title:"How Was Concentric Finance Exploited?",description:"Learn how Concentric Finance was exploited, resulting in a loss of assets worth $1.72 million.",section:"Posts",handler:()=>{window.location.href="/blog/2024/concentric-finance-exploit/"}},{id:"post-how-was-radiant-capital-exploited",title:"How Was Radiant Capital Exploited?",description:"Learn the Radiant Capital exploit due to a smart contract vulnerability, resulting in a $4.5M loss.",section:"Posts",handler:()=>{window.location.href="/blog/2024/radiant-capital-exploit/"}},{id:"post-analysis-of-the-pine-protocol-exploit",title:"Analysis of the Pine Protocol Exploit",description:"Learn how Pine Protocol was exploited, resulting in a loss of assets worth 40 ETH.",section:"Posts",handler:()=>{window.location.href="/blog/2023/pine-protocol-exploit/"}},{id:"post-analysis-of-the-transit-finance-exploit",title:"Analysis of the Transit Finance Exploit",description:"Learn how Transit Finance was exploited, resulting in a loss of assets worth $115,000.",section:"Posts",handler:()=>{window.location.href="/blog/2023/transit-finance-exploit/"}},{id:"post-analysis-of-supply-chain-attack-on-ledger",title:"Analysis of Supply Chain Attack on Ledger",description:"Learn how the supply chain attack on Ledger was exploited to affect numerous dApps.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ledger-exploit/"}},{id:"post-how-was-bearn-dao-exploited",title:"How Was Bearn DAO Exploited?",description:"Learn how Bearn DAO was exploited, resulting in a loss of assets worth $769,000.",section:"Posts",handler:()=>{window.location.href="/blog/2023/bearn-dao-exploit/"}},{id:"post-analysis-of-the-overflow-bug-in-bitcoin",title:"Analysis of the Overflow Bug in Bitcoin",description:"Learn more about the bitcoin overflow issue and its consequences for the network.",section:"Posts",handler:()=>{window.location.href="/blog/2023/bitcoin-integer-overflow/"}},{id:"post-how-was-raft-protocol-exploited",title:"How Was Raft Protocol Exploited?",description:"Learn how the Raft Protocol was exploited, leading to a loss of assets worth 1577 ETH.",section:"Posts",handler:()=>{window.location.href="/blog/2023/raft-protocol-exploit/"}},{id:"post-taking-a-closer-look-at-trustpad-exploit",title:"Taking a Closer Look At Trustpad Exploit",description:"Learn how the TrustPad was exploited, leading to a loss of 615 BNB worth $152,000.",section:"Posts",handler:()=>{window.location.href="/blog/2023/trustpad-exploit/"}},{id:"post-analysis-of-the-platypus-finance-exploit",title:"Analysis of the Platypus Finance Exploit",description:"Learn how Platypus Finance was exploited, resulting in a loss of funds worth $2.2 million.",section:"Posts",handler:()=>{window.location.href="/blog/2023/platypus-finance-exploit/"}},{id:"post-analysis-of-the-beluga-protocol-exploit",title:"Analysis of the Beluga Protocol Exploit",description:"Learn how the Beluga Protocol was exploited, resulting in a loss of funds worth $175,000.",section:"Posts",handler:()=>{window.location.href="/blog/2023/beluga-protocol-exploit/"}},{id:"post-analysis-of-the-stars-arena-exploit",title:"Analysis of the Stars Arena Exploit",description:"Learn how Stars Arena was exploited, resulting in a loss of funds worth $2.9 million.",section:"Posts",handler:()=>{window.location.href="/blog/2023/stars-arena-exploit/"}},{id:"post-a-comprehensive-guide-to-supply-chain-attacks-in-the-web3-world",title:"A Comprehensive Guide To Supply Chain Attacks In The Web3 World",description:"A Comprehensive Guide To Supply Chain Attacks In The Web3 World",section:"Posts",handler:()=>{window.location.href="/blog/2023/supply-chain-attack/"}},{id:"socials-telegram",title:"Telegram",section:"Socials",handler:()=>{window.open("https://telegram.me/boredpukar","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=MKlCVIIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/iampukar","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/iampukar","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/boredpukar","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>